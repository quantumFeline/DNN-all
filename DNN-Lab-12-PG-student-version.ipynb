{"cells":[{"cell_type":"markdown","id":"39250c4d","metadata":{"id":"39250c4d"},"source":["# Policy Gradient\n","\n","In reinforcement learning, the goal is to find a policy $\\pi_\\theta(a|s)$ that maximizes the expected cumulative reward. Policy Gradient (PG) methods are a class of algorithms for finding such policies in which the policy is represented by a parameterized function, such as a neural network. The key idea behind PG methods is that the gradient of the expected reward can be rephrased as the expected gradient of rewards times log-probabilities (the log-d trick):\n","\n","$$ \\nabla_\\theta\\,\\operatorname*{\\mathbb{E}}_{s,\\ a \\sim \\pi_\\theta}\\, \\text{rewards} = \\operatorname*{\\mathbb{E}}_{s,\\ a \\sim \\pi_\\theta}\\, \\text{rewards} \\cdot \\nabla_\\theta \\log \\pi_\\theta(a|s) $$\n","\n","PG methods differ in:\n","- how $ \\operatorname*{\\mathbb{E}}_{s,\\ a \\sim \\pi_\\theta}$ is defined and approximated (how trajectory samples are collected and batched in gradient descent steps),\n","- what expression is used for *rewards* (with relative advantage estimates, value functions, etc.).\n","\n","In its simplest form, we estimate the gradient with a sample of recent transitions, which were sampled according to the policy, and the PG is equal to:\n","\n","$$\n","\\nabla_{\\theta} J = \\underset{s \\sim p^{\\pi}_{*}}{\\mathbb{E}} ~ \\underset{a \\sim \\pi}{\\mathbb{E}} ~ Q^{\\pi} (s, a) ~ \\nabla_{\\theta} \\log \\pi_{\\theta} (a | s)\n","$$\n","\n","Where:\n","- $p^{\\pi}_{*}$ is the distribution of states obtained by rolling out the policy $\\pi_{\\theta}(a|s)$.\n","- $Q^{\\pi} (s, a)$ denotes the discounted Q-value under policy.\n","\n","Other popular PG variants are the Actor-Critic and PPO.\n","\n","Tasks:\n","\n","1. Run the **PolicyGradient** agent and plot mean+std of rewards (as it progresses through training).\n","2. Implement the *compute_pseudo_loss* method for the **BaselinedPolicyGradient** class.\n","3. Implement the *compute_pseudo_loss* method for the **ActorCritic** class.\n","\n","Each training takes ~3 minutes on Colab CPU (GPUs won't help)."]},{"cell_type":"code","execution_count":1,"id":"a318632a","metadata":{"id":"a318632a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1767801908129,"user_tz":-60,"elapsed":11140,"user":{"displayName":"Veronika Nechaieva","userId":"13541061591014079018"}},"outputId":"f9f0b634-5535-4565-b77b-d703750d478c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.2)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (2.0.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (3.1.2)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (4.15.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n"]}],"source":["%pip install gymnasium"]},{"cell_type":"code","execution_count":2,"id":"8ceeec92","metadata":{"id":"8ceeec92","executionInfo":{"status":"ok","timestamp":1767801912339,"user_tz":-60,"elapsed":4201,"user":{"displayName":"Veronika Nechaieva","userId":"13541061591014079018"}}},"outputs":[],"source":["import time\n","from dataclasses import dataclass\n","from typing import SupportsFloat\n","\n","import gymnasium as gym\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch import Tensor"]},{"cell_type":"code","execution_count":3,"id":"e61edada","metadata":{"id":"e61edada","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1767801912372,"user_tz":-60,"elapsed":7,"user":{"displayName":"Veronika Nechaieva","userId":"13541061591014079018"}},"outputId":"878e818f-a7d5-487f-9469-0742b31e0430"},"outputs":[{"output_type":"stream","name":"stdout","text":["True cuda\n"]}],"source":["# Check for CUDA / MPS (Apple) / XPU (Intel) / ... accelerator.\n","device = torch.accelerator.current_accelerator(True) or torch.device(\"cpu\")\n","use_accel = device != torch.device(\"cpu\")\n","print(use_accel, device)"]},{"cell_type":"code","source":["import tqdm"],"metadata":{"id":"JjwGsdaqMuuq","executionInfo":{"status":"ok","timestamp":1767801912385,"user_tz":-60,"elapsed":10,"user":{"displayName":"Veronika Nechaieva","userId":"13541061591014079018"}}},"id":"JjwGsdaqMuuq","execution_count":4,"outputs":[]},{"cell_type":"markdown","id":"f7db94c6","metadata":{"id":"f7db94c6"},"source":["## Configuration"]},{"cell_type":"code","execution_count":5,"id":"6f47d27a","metadata":{"id":"6f47d27a","executionInfo":{"status":"ok","timestamp":1767801912417,"user_tz":-60,"elapsed":29,"user":{"displayName":"Veronika Nechaieva","userId":"13541061591014079018"}}},"outputs":[],"source":["@dataclass\n","class Config:\n","    # Do not change!\n","\n","    gym_id: str = \"CartPole-v1\"\n","    buffer_size: int = 32  # Number of transitions to store in the transition buffer.\n","    hidden_dim: int = 32  # Size of hidden layer in policy and value networks.\n","    policy_learning_rate: float = 5e-3\n","    value_learning_rate: float = 1e-2\n","    discount: float = 0.99  # Gamma discount factor in definition of Q and V values.\n","    n_train_steps: int = 10_000\n","    n_trainings: int = 10\n","    n_eval_samples: int = 20  # Number of episodes rolled-out in each evaluation.\n","    steps_between_evals: int = 2000  # How often to evaluate during training.\n","    device: torch.device = device\n","\n","    def __post_init__(self):\n","        env = gym.make(self.gym_id)\n","\n","        # Observation, consisting of: position, velocity, angle, angular velocity.\n","        assert isinstance(env.observation_space, gym.spaces.Box)\n","        self.obs_size = env.observation_space.shape[0]\n","\n","        # Two actions: left and right.\n","        assert isinstance(env.action_space, gym.spaces.Discrete)\n","        self.action_size = int(env.action_space.n)\n","\n","\n","config = Config()"]},{"cell_type":"markdown","id":"1265f784","metadata":{"id":"1265f784"},"source":["## Plotter"]},{"cell_type":"code","execution_count":18,"id":"4afd2b81","metadata":{"id":"4afd2b81","executionInfo":{"status":"ok","timestamp":1767803189848,"user_tz":-60,"elapsed":3,"user":{"displayName":"Veronika Nechaieva","userId":"13541061591014079018"}}},"outputs":[],"source":["class Plotter:\n","    \"\"\"Collects and plots rewards curves for each experiment.\"\"\"\n","\n","    def __init__(self, config: Config) -> None:\n","        self.config = config\n","        # Maps experiment name to list of curves (from different training seeds).\n","        # Each curve is a list of mean total rewards in evaluations done during training.\n","        self.data = dict[str, list[list[float]]]()\n","\n","    def reset(self) -> None:\n","        self.data = dict[str, list[list[float]]]()\n","\n","    def add_rewards_curve(self, label: str, rewards_curve: list[float]) -> None:\n","        \"\"\"Plot any number of reward curves, .\"\"\"\n","        if label not in self.data:\n","            self.data[label] = []\n","        self.data[label].append(rewards_curve)\n","\n","    def show(self) -> None:\n","        ## TODO {\n","        experiments = self.data.keys()\n","        average_traj = np.mean(np.array([self.data[experiment] for experiment in tqdm.tqdm(experiments)]), axis=1)\n","        plt.plot(average_traj)\n","        ## }\n","        plt.ylim(bottom=0, top=400)\n","        plt.legend(loc=\"upper left\")\n","        plt.xlabel(\"Training Progress [%]\")\n","        plt.ylabel(\"Reward (mean across trainings)\")\n","        plt.show()\n","\n","\n","plotter = Plotter(config)"]},{"cell_type":"markdown","id":"263c8775","metadata":{"id":"263c8775"},"source":["## Networks"]},{"cell_type":"markdown","id":"d6a2a605","metadata":{"id":"d6a2a605"},"source":["We define a policy network (**ActorNetwork**) and a value network (**CriticNetwork**), with helper methods to work directly with numpy.ndarray observations."]},{"cell_type":"code","execution_count":11,"id":"8d97a2b8","metadata":{"id":"8d97a2b8","executionInfo":{"status":"ok","timestamp":1767802075301,"user_tz":-60,"elapsed":16,"user":{"displayName":"Veronika Nechaieva","userId":"13541061591014079018"}}},"outputs":[],"source":["class ActorNetwork(nn.Module):\n","    def __init__(self, config: Config) -> None:\n","        super().__init__()\n","        self.layers = nn.Sequential(\n","            nn.Linear(config.obs_size, config.hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(config.hidden_dim, config.action_size),\n","        )\n","\n","    def forward(self, obs: Tensor) -> Tensor:\n","        \"\"\"\n","        Input: observation, shape (batch_size, obs_size).\n","        Output: logits for action probabilities, shape (batch_size, action_size).\n","        \"\"\"\n","        return self.layers(obs)  # No softmax here.\n","\n","    def sample_action(self, obs: np.ndarray) -> tuple[Tensor, Tensor]:\n","        \"\"\"\n","        Sample an action for a given observation of shape (batch_size, obs_size).\n","\n","        Returns:\n","        - action: sampled action, shape (batch_size,), dtype int.\n","        - logprob: log prob. of that action, shape (batch_size,), differentiable wrt actor params.\n","        \"\"\"\n","        batch_size, _obs_size = obs.shape\n","        device = next(self.parameters()).device\n","        logits = self(torch.tensor(obs, device=device))\n","        dist = torch.distributions.Categorical(logits=logits)\n","        action = dist.sample((batch_size,))\n","        return action, dist.log_prob(action)\n","\n","\n","class CriticNetwork(nn.Module):\n","    def __init__(self, config: Config) -> None:\n","        super().__init__()\n","        self.layers = nn.Sequential(\n","            nn.Linear(config.obs_size, config.hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(config.hidden_dim, 1),\n","        )\n","\n","    def forward(self, obs: Tensor) -> Tensor:\n","        \"\"\"\n","        Input: observation, shape (batch_size, obs_size).\n","        Output: V value estimate, shape (batch_size,).\n","        \"\"\"\n","        return torch.squeeze(self.layers(obs), -1)\n","\n","    def get_value(self, obs: np.ndarray) -> Tensor:\n","        \"\"\"Given obs of shape (batch_size, obs_size), return value estimate, shape (batch_size,).\"\"\"\n","        device = next(self.parameters()).device\n","        return self(torch.tensor(obs, device=device))"]},{"cell_type":"markdown","id":"183095c8","metadata":{"id":"183095c8"},"source":["## Buffer\n","We maintain a buffer (a.k.a. batch) of recent transitions, to make updates averaged over multiple transitions.\n","The buffer is small (buffer_size=32 vs episode length ~200), not like the experience replay buffer from DQN.\n","We add to it until it is full, use it to make an update, and then clear it; repeat.\n","\n","The buffer defines a method for calculating Q-values: as sums of rewards (discounted).\n","Since the buffer is small, we will never have a full episode in the buffer, so we need an estimate of rewards post final state in the buffer.\n","We get such an estimate from the value network (critic).\n","This will be the only use of the value network, for the first algorithm."]},{"cell_type":"code","execution_count":8,"id":"39231fdd","metadata":{"id":"39231fdd","executionInfo":{"status":"ok","timestamp":1767801912573,"user_tz":-60,"elapsed":49,"user":{"displayName":"Veronika Nechaieva","userId":"13541061591014079018"}}},"outputs":[],"source":["class Buffer:\n","    def __init__(self, buffer_size: int, device: torch.device) -> None:\n","        self.buffer_size = buffer_size\n","        self.device = device\n","        self.reset()\n","\n","    def reset(self) -> None:\n","        buffer_size, device = self.buffer_size, self.device\n","        self.logprobs = torch.zeros(buffer_size, device=device)\n","        self.values = torch.zeros(buffer_size, device=device)\n","        self.rewards = torch.zeros(buffer_size, device=device)\n","        self.terminals = torch.zeros(buffer_size, dtype=torch.bool, device=device)\n","        self.idx = 0\n","\n","    def add(\n","        self,\n","        logprob: Tensor,\n","        value: Tensor,\n","        reward: SupportsFloat,\n","        terminal: SupportsFloat,\n","    ) -> None:\n","        \"\"\"Called after each train episode step (with differentiable logprob and value scalars).\"\"\"\n","        self.logprobs[self.idx] = logprob\n","        self.values[self.idx] = value\n","        self.rewards[self.idx] = float(reward)\n","        self.terminals[self.idx] = float(terminal)\n","        self.idx += 1\n","\n","    def get_qvalues(self, final_value: Tensor, discount: float = 1.0) -> Tensor:\n","        \"\"\"\n","        Return Q-values for the buffer (discounted sum of rewards), shape (buffer_size,).\n","\n","        Args:\n","        - final_value: V value estimate for the final state, scalar.\n","        - discount: gamma in the definition of Q-values.\n","        \"\"\"\n","        q_values = torch.zeros_like(self.rewards).to(self.rewards.device)\n","\n","        next_value = final_value\n","        for timestep in reversed(range(self.buffer_size)):\n","            if self.terminals[timestep]:\n","                next_value = 0.0\n","\n","            q_values[timestep] = self.rewards[timestep] + discount * next_value\n","            next_value = q_values[timestep]\n","\n","        return q_values"]},{"cell_type":"markdown","id":"4ec42f68","metadata":{"id":"4ec42f68"},"source":["## PolicyGradient trainer"]},{"cell_type":"code","execution_count":9,"id":"585f0e73","metadata":{"id":"585f0e73","executionInfo":{"status":"ok","timestamp":1767801912575,"user_tz":-60,"elapsed":18,"user":{"displayName":"Veronika Nechaieva","userId":"13541061591014079018"}}},"outputs":[],"source":["class PolicyGradient:\n","    def __init__(self, config: Config) -> None:\n","        super().__init__()\n","        self.config = config\n","        self.buffer = Buffer(config.buffer_size, device=config.device)\n","        self.init_networks()\n","\n","    def init_networks(self) -> None:\n","        self.actor = ActorNetwork(self.config).to(self.config.device)\n","        self.critic = CriticNetwork(self.config).to(self.config.device)\n","        self.actor_optimizer = optim.Adam(\n","            self.actor.parameters(),\n","            lr=self.config.policy_learning_rate,\n","        )\n","        self.critic_optimizer = optim.Adam(\n","            self.critic.parameters(),\n","            lr=self.config.value_learning_rate,\n","        )\n","        self.buffer.reset()\n","\n","    def evaluate(self) -> float:\n","        \"\"\"Rollout `samples` episodes. Return the average total reward.\"\"\"\n","        self.actor.eval()\n","        self.critic.eval()\n","        with torch.no_grad():\n","            env_test = gym.make(self.config.gym_id)\n","            total_reward = 0.0\n","            for _ in range(self.config.n_eval_samples):\n","                obs, _info = env_test.reset()\n","                episode_reward = 0.0\n","                while True:\n","                    action, _logprobs = self.actor.sample_action(obs[np.newaxis, :])\n","                    next_obs, reward, terminal, truncated, _info = env_test.step(\n","                        action.item()\n","                    )\n","                    done = terminal or truncated\n","                    episode_reward += float(reward)\n","                    obs = next_obs\n","                    if done:\n","                        break\n","                total_reward += episode_reward\n","        return total_reward / self.config.n_eval_samples\n","\n","    def train(self, seed: int) -> list[float]:\n","        \"\"\"Trains the agent and returns (timesteps // steps_between_evals) evaluation rewards.\"\"\"\n","        env = gym.make(self.config.gym_id)\n","        obs, _ = env.reset(seed=seed)\n","\n","        start_time = time.time()\n","        eval_rewards = list[float]()\n","\n","        for step in range(config.n_train_steps):\n","            self.actor.train()\n","            self.critic.train()\n","\n","            value = self.critic.get_value(obs[np.newaxis, :])\n","            action, logprob = self.actor.sample_action(obs[np.newaxis, :])\n","            next_obs, reward, terminal, truncated, _ = env.step(action.item())\n","            done = terminal or truncated\n","            self.buffer.add(logprob, value, reward, done)\n","            obs = next_obs\n","\n","            if self.buffer.idx == self.config.buffer_size:\n","                self.update(obs)\n","\n","            if done:\n","                obs, _info = env.reset(seed=seed)\n","\n","            # Evaluate every steps_between_evals.\n","            if (step + 1) % self.config.steps_between_evals == 0:\n","                eval_reward = self.evaluate()\n","                eval_rewards.append(eval_reward)\n","                samples_per_sec = (step + 1) / (time.time() - start_time)\n","                if (step + 1) % (10 * self.config.steps_between_evals) == 0:\n","                    print(f\"{step+1=}, {eval_reward=:.1f}, {samples_per_sec=:.0f}\")\n","\n","        return eval_rewards\n","\n","    def update(self, final_obs: np.ndarray) -> None:\n","        # Final-value and hence target q-values depend on critic parameters,\n","        # but we do not want to backprop through it\n","        # (neither in the critic nor in the actor update)!\n","        with torch.no_grad():\n","            final_value = self.critic.get_value(final_obs[np.newaxis, :])\n","            q_values = self.buffer.get_qvalues(final_value, self.config.discount)\n","\n","        self.critic_optimizer.zero_grad()\n","        value_loss = (self.buffer.values - q_values).pow(2).mean()\n","        value_loss.backward()\n","        self.critic_optimizer.step()\n","\n","        self.actor_optimizer.zero_grad()\n","        pseudo_loss = self.compute_pseudo_loss(q_values)\n","        pseudo_loss.backward()\n","        self.actor_optimizer.step()\n","\n","        self.buffer.reset()\n","\n","    def compute_pseudo_loss(self, q_values: Tensor) -> Tensor:\n","        \"\"\"Compute pseudo-loss for the policy (actor) network, using the collected buffer.\"\"\"\n","        return -(q_values * self.buffer.logprobs).mean()"]},{"cell_type":"code","execution_count":20,"id":"w2Y3YZFFQVRz","metadata":{"id":"w2Y3YZFFQVRz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1767803505178,"user_tz":-60,"elapsed":302719,"user":{"displayName":"Veronika Nechaieva","userId":"13541061591014079018"}},"outputId":"153d7655-2513-4b52-9a5a-8f15db334f07"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [05:02<00:00, 30.27s/it]\n"]}],"source":["for i in tqdm.tqdm(range(config.n_trainings)):\n","    plotter.add_rewards_curve(\"vanilla\", PolicyGradient(config).train(i))\n"]},{"cell_type":"code","source":["plotter.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":304},"id":"Gft_prJGR591","executionInfo":{"status":"error","timestamp":1767803195097,"user_tz":-60,"elapsed":35,"user":{"displayName":"Veronika Nechaieva","userId":"13541061591014079018"}},"outputId":"d56420ce-a845-4f35-a3a8-829b177aebaf"},"id":"Gft_prJGR591","execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["0it [00:00, ?it/s]\n"]},{"output_type":"error","ename":"AxisError","evalue":"axis 1 is out of bounds for array of dimension 1","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2785434003.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplotter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-3070023527.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m## TODO {\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mexperiments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0maverage_traj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mexperiment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverage_traj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m## }\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m   3594\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3596\u001b[0;31m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0m\u001b[1;32m   3597\u001b[0m                           out=out, **kwargs)\n\u001b[1;32m   3598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0mrcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_count_reduce_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrcount\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mumr_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrcount\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mean of empty slice.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRuntimeWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py\u001b[0m in \u001b[0;36m_count_reduce_items\u001b[0;34m(arr, axis, keepdims, where)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mitems\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_axis_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"]}]},{"cell_type":"markdown","id":"270d8fc8","metadata":{"id":"270d8fc8"},"source":["## PG with baseline"]},{"cell_type":"markdown","id":"c0f33e74","metadata":{"id":"c0f33e74"},"source":["The vanilla PG estimate has big variance – it will vary a lot depending on the states and actions sampled.\n","This variance is known to impact the sample efficiency and final performance of the agent.\n","Although there are many strategies fo PG variance reduction, there is one trick that became indispensable in modern implementations of PG – the baseline. But before we get to this, lets have one more look at the vanilla PG estimate:\n","\n","$$\n","\\nabla_{\\theta} J = \\underset{s \\sim p^{\\pi}_{*}}{\\mathbb{E}} ~ \\underset{a \\sim \\pi}{\\mathbb{E}} ~ Q^{\\pi} (s, a) ~ \\nabla_{\\theta} \\log \\pi_{\\theta} (a | s)\n","$$\n","\n","As discussed in the lecture, the idea of PG is quite simple – increase the probability of good actions and decrease the probability of bad ones.\n","'Good' and 'bad' here refer to Q-values associated with given actions. But what if all Q-values are positive? The gradient update will try to increase the logits of all sampled actions, with the increase being proportional to the Q-value (as stems from the equation above). As such, given positive Q-values, the logits of bad actions are also increased – just by a smaller amount that logits of actions with bigger Q-values.\n","\n","Baseline variance reduction tackles exactly that. The idea is to subtract a \"baseline\" from the Q-values, which does not affect the optimal policy, but reduces the variance of the gradients. It is proven that as long as the baseline does not depend on the action, its value will not bias the PG (a bad choice may increase the variance, though :)). So, what is a good baseline?\n","\n","The simplest version subtracts batch average of Q-values from each Q-value:\n","\n","$$\n","\\nabla_{\\theta} J = \\sum_{i=1}^{B} \\frac{1}{B} \\Bigl( Q^{\\pi} (s_i, a_i) - \\frac{\\sum Q^{\\pi} (s_i, a_i)}{B} \\Bigr) \\nabla_{\\theta} \\log \\pi_{\\theta} (a_i | s_i)\n","$$\n","\n","Where B is the batch size and $\\frac{\\sum Q^{\\pi} (s_i, a_i)}{B}$ is the average Q-value in the batch."]},{"cell_type":"code","execution_count":13,"id":"4a145a52","metadata":{"id":"4a145a52","executionInfo":{"status":"ok","timestamp":1767802379351,"user_tz":-60,"elapsed":51,"user":{"displayName":"Veronika Nechaieva","userId":"13541061591014079018"}}},"outputs":[],"source":["class BaselinedPolicyGradient(PolicyGradient):\n","    def compute_pseudo_loss(self, q_values: Tensor) -> Tensor:\n","        ## TODO {\n","        average = q_values.mean()\n","        baseline = q_values - average\n","        return -(baseline * self.buffer.logprobs).mean()\n","        ## }"]},{"cell_type":"code","execution_count":null,"id":"0bf6d9f9","metadata":{"id":"0bf6d9f9","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3e9c6001-0004-4b12-f646-b14e83042082"},"outputs":[{"output_type":"stream","name":"stderr","text":[" 20%|██        | 2/10 [01:08<04:34, 34.27s/it]"]}],"source":["for i in tqdm.tqdm(range(config.n_trainings)):\n","    plotter.add_rewards_curve(\"mean baseline\", BaselinedPolicyGradient(config).train(i))\n","plotter.show()"]},{"cell_type":"markdown","id":"8f6bd3e0","metadata":{"id":"8f6bd3e0"},"source":["## Actor-Critic"]},{"cell_type":"markdown","id":"729f5f80","metadata":{"id":"729f5f80"},"source":["The most popular approach is to use the state V value, estimated by a value network (*CriticNetwork*), as the baseline. Then, the gradient is well centered – probabilities of actions with Q-values smaller than state value will be decreased, while probabilities of actions with Q-values bigger than state value will be increased. Traditionally, PG variants that use a value network for baseline variance reduction are called Actor-Critic. The Actor-Critic update is calculated as:\n","\n","$$\n","\\nabla_{\\theta} J = \\sum_{i=1}^{B} \\frac{1}{B} \\bigl( Q^{\\pi} (s_i, a_i) - V_{\\phi}(s) \\bigr) \\nabla_{\\theta} \\log \\pi_{\\theta} (a_i | s_i)\n","$$\n","\n","Where $\\bigl( Q^{\\pi} (s_i, a_i) - V_{\\phi}(s) \\bigr)$ is referred to as the **advantage**."]},{"cell_type":"code","execution_count":21,"id":"261d7dfc","metadata":{"id":"261d7dfc","executionInfo":{"status":"ok","timestamp":1767803505451,"user_tz":-60,"elapsed":9,"user":{"displayName":"Veronika Nechaieva","userId":"13541061591014079018"}}},"outputs":[],"source":["class ActorCritic(PolicyGradient):\n","    def compute_pseudo_loss(self, q_values: Tensor) -> Tensor:\n","        ## TODO {\n","        critic_network = self.critic\n","        v_value = critic_network.get_value(self.buffer.values)\n","        advantage = q_values - v_value\n","        return -(advantage * self.buffer.logprobs).mean()\n","        ##}"]},{"cell_type":"code","execution_count":null,"id":"4df0c7c4","metadata":{"id":"4df0c7c4"},"outputs":[],"source":["for i in range(config.n_trainings):\n","    plotter.add_rewards_curve(\"V baseline\", ActorCritic(config).train(i))\n","plotter.show()"]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/mim-ml-teaching/public-dnn-2025-26/blob/master/docs/DNN-Lab-12-PG-student-version.ipynb","timestamp":1767799623600}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"}},"nbformat":4,"nbformat_minor":5}