{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84VetyCaGLyR"
      },
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1_utx_ZGclmCwNttSe40kYA6VHzNocdET' height=\"60\"></center>\n",
        "\n",
        "AI TECH - Akademia Innowacyjnych Zastosowań Technologii Cyfrowych. Program Operacyjny Polska Cyfrowa na lata 2014-2020\n",
        "<hr>\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?id=1BXZ0u3562N_MqCLcekI-Ens77Kk4LpPm'></center>\n",
        "\n",
        "<center>\n",
        "Projekt współfinansowany ze środków Unii Europejskiej w ramach Europejskiego Funduszu Rozwoju Regionalnego\n",
        "Program Operacyjny Polska Cyfrowa na lata 2014-2020,\n",
        "Oś Priorytetowa nr 3 \"Cyfrowe kompetencje społeczeństwa\" Działanie  nr 3.2 \"Innowacyjne rozwiązania na rzecz aktywizacji cyfrowej\"\n",
        "Tytuł projektu:  „Akademia Innowacyjnych Zastosowań Technologii Cyfrowych (AI Tech)”\n",
        "    </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziZ9i7tXbO1T"
      },
      "source": [
        "In this lab, you will implement some of the techniques discussed in the lecture.\n",
        "\n",
        "Below you are given a solution to the previous scenario. It has two serious drawbacks:\n",
        " * The output predictions do not sum up to one (i.e. the output is not a probability distribution), even though the images always contain exactly one digit.\n",
        " * It uses MSE coupled with output sigmoid, which can lead to saturation and slow convergence.\n",
        "\n",
        "**Task 0.** Implement a numerically stable version of softmax.\n",
        "\n",
        "**Task 1.** Use softmax instead of coordinate-wise sigmoid and use log-loss instead of MSE. Test to see if this improves convergence. Hint: When implementing backprop it might be easier to consider these two functions as a single block, rather than compute the gradient over the softmax values.\n",
        "\n",
        "**Task 2.** Implement L2 regularization and add momentum to the SGD algorithm. Play with different amounts of regularization and momentum. See if this improves accuracy/convergence.\n",
        "\n",
        "**Task 3 (optional).** Implement Adagrad or AdamW (currently popular in LLM training), dropout, and some simple data augmentations (e.g. tiny rotations/shifts, etc.). Again, test to see how these changes improve accuracy/convergence.\n",
        "\n",
        "**Task 4.** Try adding extra layers to the network. Again, test how the changes you introduced affect accuracy/convergence. As a start, you can try this architecture: [784,100,30,10]\n",
        "\n",
        "The provided model evaluation code (`evaluate_model`) may take some time to complete. During implementation, you can change the number of evaluated models to 1 and reduce the number of tested learning rates, and epochs.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZypYewcXywA",
        "outputId": "81aa43d6-732f-4694-82d3-01111e293465"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "2026-02-06 16:41:48 URL:https://s3.amazonaws.com/img-datasets/mnist.npz [11490434/11490434] -> \"mnist.npz\" [1]\n"
          ]
        }
      ],
      "source": [
        "!pip install tqdm pandas\n",
        "!wget --no-verbose -O mnist.npz https://s3.amazonaws.com/img-datasets/mnist.npz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "P22HqX9AbO1a"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Any, Callable, Sequence\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "from numpy.typing import NDArray\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "FloatNDArray = NDArray[np.float64]\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "N9jGPaZhbO2B"
      },
      "outputs": [],
      "source": [
        "def load_mnist(\n",
        "    path: Path = Path(\"mnist.npz\")\n",
        ") -> tuple[FloatNDArray, FloatNDArray, FloatNDArray, FloatNDArray]:\n",
        "    \"\"\"\n",
        "    Load the MNIST dataset (grayscale 28 x 28 images of hand-written digits).\n",
        "\n",
        "    Returns tuple of:\n",
        "    - x_train: shape (N_train, H * W), grayscale values 0..1.\n",
        "    - y_train: shape (N_train, 10), one-hot-encoded label, dtype float64.\n",
        "    - x_test: shape (N_test, H * W), grayscale values 0..1.\n",
        "    - y_train: shape (N_test, 10), one-hot-encoded label, dtype float64.\n",
        "\n",
        "    More: https://en.wikipedia.org/wiki/MNIST_database\n",
        "    \"\"\"\n",
        "    with np.load(path) as f:\n",
        "        x_train, _y_train = f[\"x_train\"], f[\"y_train\"]\n",
        "        x_test, _y_test = f[\"x_test\"], f[\"y_test\"]\n",
        "\n",
        "    H = W = 28\n",
        "    N_train = len(x_train)\n",
        "    N_test = len(x_test)\n",
        "    assert x_train.shape == (N_train, H, W) and _y_train.shape == (N_train,)\n",
        "    assert x_test.shape == (N_test, H, W) and _y_test.shape == (N_test,)\n",
        "\n",
        "    x_train = x_train.reshape(N_train, H * W) / 255.0\n",
        "    x_test = x_test.reshape(N_test, H * W) / 255.0\n",
        "\n",
        "    y_train = np.zeros((N_train, 10), dtype=np.float64)\n",
        "    y_train[np.arange(N_train), _y_train] = 1\n",
        "\n",
        "    y_test = np.zeros((N_test, 10))\n",
        "    y_test[np.arange(N_test), _y_test] = 1\n",
        "\n",
        "    return x_train, y_train, x_test, y_test\n",
        "\n",
        "x_train, y_train, x_test, y_test = load_mnist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "w3gAyqw4bO1p"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z: FloatNDArray) -> FloatNDArray:\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "\n",
        "def sigmoid_prime(z: FloatNDArray) -> FloatNDArray:\n",
        "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
        "    return sigmoid(z) * (1 - sigmoid(z))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvVG90fCXywB"
      },
      "source": [
        "## Warm-Up\n",
        "Implement a numerically stable version of softmax.  \n",
        "\n",
        "In general, softmax is defined as  \n",
        "$$\\text{softmax}(x_1, x_2, \\ldots, x_n) = (\\frac{e^{x_1}}{\\sum_i{e^{x_i}}}, \\frac{e^{x_2}}{\\sum_i{e^{x_i}}}, \\ldots, \\frac{e^{x_n}}{\\sum_i{e^{x_i}}})$$  \n",
        "However, taking $e^{1000000}$ can result in NaN.  \n",
        "Can you implement softmax so that the highest power to which e will be risen will be at most $0$ and the predictions will be mathematically equivalent?  \n",
        "\n",
        "Hint: <sub><sub><sub>sǝnlɐʌ llɐ ɯoɹɟ ʇᴉ ʇɔɐɹʇqns  puɐ ᴉ‾x ʇsǝƃɹɐl ǝɥʇ ǝʞɐʇ</sub></sub></sub>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rutQhoaXywC",
        "outputId": "937566ed-909e-44ea-c48a-37ad4b748842"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK\n",
            "[2.78946778e-10 2.78946778e-10 9.99999887e-01 1.12535162e-07\n",
            " 9.35762191e-14]\n",
            "[2.78946778e-10 2.78946778e-10 9.99999887e-01 1.12535162e-07\n",
            " 9.35762191e-14]\n",
            "1.0\n"
          ]
        }
      ],
      "source": [
        "def unstable_softmax(x: FloatNDArray, axis: int = -1) -> FloatNDArray:\n",
        "    e = np.exp(x)\n",
        "    return e / np.sum(e, axis=axis, keepdims=True)\n",
        "\n",
        "\n",
        "def stable_softmax(x: FloatNDArray, axis: int = -1) -> FloatNDArray:\n",
        "    e = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
        "    return e / np.sum(e, axis=axis, keepdims=True)\n",
        "\n",
        "# def softmax_prime(x: FloatNDArray) -> FloatNDArray:\n",
        "#     return stable_softmax(x) * (1 - stable_softmax(x))\n",
        "\n",
        "### TESTS ###\n",
        "def _test_one(x: FloatNDArray, y: FloatNDArray) -> None:\n",
        "    r = stable_softmax(x)\n",
        "    assert r.shape == y.shape, f\"Expected shape {y.shape}, got {r.shape=}\"\n",
        "    assert np.isclose(np.ones(x.shape[0]), r.sum(axis=-1), atol=1e-5, rtol=0).all()\n",
        "    assert np.isclose(y, r, atol=1e-5, rtol=0).all()\n",
        "\n",
        "def test_stable_softmax() -> None:\n",
        "    x1 = np.random.rand(100, 32).astype(np.float64)\n",
        "    _test_one(x1, unstable_softmax(x1))\n",
        "\n",
        "    x2 = np.ones((10, 10, 32), dtype=np.float64) * 1e6\n",
        "    _test_one(x2, np.ones_like(x2) / x2.shape[-1])\n",
        "\n",
        "    print(\"OK\")\n",
        "\n",
        "test_stable_softmax()\n",
        "print(unstable_softmax((10, 10, 32, 16, 2)))\n",
        "print(stable_softmax((10, 10, 32, 16, 2)))\n",
        "print(np.sum(stable_softmax((10, 10, 32, 16, 2))))\n",
        "### TESTS END ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWFnNWF8fWJB"
      },
      "source": [
        "## ModelResults\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FgEA2XRRbO2X"
      },
      "outputs": [],
      "source": [
        "class ModelResults:\n",
        "    \"\"\"Just a helper class for gathering results in a nice table. Feel free to ignore.\"\"\"\n",
        "    def __init__(self):\n",
        "        # Map from model name to map from lr to list of test accuracies.\n",
        "        self.results = dict[str, dict[float, list[float]]]()\n",
        "\n",
        "    def clear(self, model_name: str | None = None) -> None:\n",
        "        \"\"\"Forget results for a given model (defaults to all models).\"\"\"\n",
        "        if model_name:\n",
        "            if model_name in self.results:\n",
        "                del self.results[model_name]\n",
        "        else:\n",
        "            self.results = {}\n",
        "\n",
        "    def add_result(self, model_name: str, learning_rate: float, accuracy: float) -> None:\n",
        "        if model_name not in self.results:\n",
        "            self.results[model_name] = {}\n",
        "        if learning_rate not in self.results[model_name]:\n",
        "            self.results[model_name][learning_rate] = []\n",
        "        self.results[model_name][learning_rate].append(accuracy)\n",
        "\n",
        "    def display_results(self) -> None:\n",
        "        data = list[dict[str, Any]]()\n",
        "        for model_name, model_results in self.results.items():\n",
        "            for lr, accuracies in model_results.items():\n",
        "                mean_accuracy = np.mean(accuracies)\n",
        "                accuracy_summary = f\"{mean_accuracy:2.1%} ± {np.std(accuracies) * 100:.1f} p.p.\"\n",
        "                data.append({\n",
        "                    \"model\": model_name,\n",
        "                    \"lr\": lr,\n",
        "                    \"mean_accuracy\": mean_accuracy,\n",
        "                    \"accuracy\": accuracy_summary\n",
        "                })\n",
        "\n",
        "        df = pd.DataFrame(data).sort_values(\"mean_accuracy\", ascending=False)\n",
        "        del df[\"mean_accuracy\"]\n",
        "        display(df.style.format({\"lr\": \"{:.1g}\"}).hide())\n",
        "\n",
        "    def evaluate_model(\n",
        "        self,\n",
        "        model_name: str,\n",
        "        model_constructor: Callable[[Sequence[int]], Any],\n",
        "        layers: Sequence[int] = (784, 30, 10),\n",
        "        learning_rates: Sequence[float] = (1.0, 10.0, 100.0),\n",
        "        n_trainings: int = 3,\n",
        "        **kwargs: Any\n",
        "    ) -> None:\n",
        "        # Automatic model name with parameters.\n",
        "        if kwargs:\n",
        "            if tuple(layers) != (784, 30, 10):\n",
        "                model_name += \"[\" + \",\".join(str(n) for n in layers) + \"]\"\n",
        "\n",
        "            model_name += \"(\"\n",
        "            for k, v in kwargs.items():\n",
        "                if isinstance(v, (float,  np.floating)):\n",
        "                    model_name += f\"{k}={v:.1g},\"\n",
        "                else:\n",
        "                    model_name += f\"{k}={v},\"\n",
        "            model_name = model_name[:-1]\n",
        "            model_name += \")\"\n",
        "\n",
        "        # Train for each learning rate, n_trainings times.\n",
        "        for lr in learning_rates:\n",
        "            print(f\"Checking {n_trainings} random trainings with with lr = {lr}\")\n",
        "            for i in range(n_trainings):\n",
        "                network = model_constructor(layers, **kwargs)\n",
        "                accuracy = network.train(\n",
        "                    (x_train, y_train),\n",
        "                    epochs=10,\n",
        "                    mini_batch_size=100,\n",
        "                    learning_rate=lr,\n",
        "                    test_data=(x_test, y_test),\n",
        "                )\n",
        "                self.add_result(model_name, lr, float(accuracy))\n",
        "\n",
        "\n",
        "model_results = ModelResults()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJD-EMvq6H2l"
      },
      "source": [
        "## Baseline\n",
        "The solution to the previous lab: an MLP network with MSE loss on sigmoid outputs, trained with plain SGD (batched)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "tnBAMfMP6IRJ",
        "outputId": "153c2692-ccc5-4442-c6b4-44997590c9ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking 3 random trainings with with lr = 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 10/10 [00:15<00:00,  1.56s/it, Test accuracy: 90.76 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:09<00:00,  1.05it/s, Test accuracy: 91.06 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:10<00:00,  1.04s/it, Test accuracy: 91.09 %]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking 3 random trainings with with lr = 10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 10/10 [00:07<00:00,  1.40it/s, Test accuracy: 94.73 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:09<00:00,  1.06it/s, Test accuracy: 94.72 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:09<00:00,  1.09it/s, Test accuracy: 94.84 %]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking 3 random trainings with with lr = 100.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 10/10 [00:07<00:00,  1.35it/s, Test accuracy: 10.32 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:09<00:00,  1.06it/s, Test accuracy: 10.09 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:07<00:00,  1.31it/s, Test accuracy: 8.92 %]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7933185e1010>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_79ec6\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th id=\"T_79ec6_level0_col0\" class=\"col_heading level0 col0\" >model</th>\n",
              "      <th id=\"T_79ec6_level0_col1\" class=\"col_heading level0 col1\" >lr</th>\n",
              "      <th id=\"T_79ec6_level0_col2\" class=\"col_heading level0 col2\" >accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td id=\"T_79ec6_row0_col0\" class=\"data row0 col0\" >Baseline</td>\n",
              "      <td id=\"T_79ec6_row0_col1\" class=\"data row0 col1\" >1e+01</td>\n",
              "      <td id=\"T_79ec6_row0_col2\" class=\"data row0 col2\" >94.8% ± 0.1 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_79ec6_row1_col0\" class=\"data row1 col0\" >Baseline</td>\n",
              "      <td id=\"T_79ec6_row1_col1\" class=\"data row1 col1\" >1</td>\n",
              "      <td id=\"T_79ec6_row1_col2\" class=\"data row1 col2\" >91.0% ± 0.1 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_79ec6_row2_col0\" class=\"data row2 col0\" >Baseline</td>\n",
              "      <td id=\"T_79ec6_row2_col1\" class=\"data row2 col1\" >1e+02</td>\n",
              "      <td id=\"T_79ec6_row2_col2\" class=\"data row2 col2\" >9.8% ± 0.6 p.p.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "class Network:\n",
        "    def __init__(self, sizes: Sequence[int] = (28*28, 30, 10)):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "        - sizes: sequence of layer widths [N^0, ... , N^last]\n",
        "          These are lengths of activation vectors, where:\n",
        "          - N^0 is input size: H * W = 28 * 28 = 784.\n",
        "          - N^last is the number of classes into which we can classify each input: 10.\n",
        "        \"\"\"\n",
        "        self.sizes = list(sizes)\n",
        "\n",
        "        # List of len(sizes) - 1 vectors of shape (N^1), (N^2), ..., (N^last).\n",
        "        self.biases = [np.random.randn(n) for n in sizes[1:]]\n",
        "\n",
        "        # List of len(sizes) - 1 matrices of shape (N^i, N^{i-1}).\n",
        "        # Weights are indexed by target node first.\n",
        "        self.weights = [\n",
        "            np.random.randn(n_out, n_in) / np.sqrt(n_in)\n",
        "            for n_in, n_out in zip(sizes[:-1], sizes[1:], strict=True)\n",
        "        ]\n",
        "\n",
        "        self.num_layers = len(self.weights)   # = len(sizes) - 1\n",
        "\n",
        "\n",
        "    def feedforward(self, x: FloatNDArray) -> FloatNDArray:\n",
        "        \"\"\"\n",
        "        Run the network on a batch of cases of shape (B, N^0), values 0..1.\n",
        "\n",
        "        Returns last layer activations, shape (B, N^last), values 0..1.\n",
        "        \"\"\"\n",
        "        g = x\n",
        "        for w, b in zip(self.weights, self.biases, strict=True):\n",
        "            # Shapes (B, N^{i-1}) @ (N^{i-1}, N^i) + (N^i,)  ==  (B, N^i)\n",
        "            g = sigmoid(g @ w.T + b)\n",
        "        return g\n",
        "\n",
        "    def learning_step(self, x_mini_batch: FloatNDArray, y_mini_batch: FloatNDArray, learning_rate: float) -> None:\n",
        "        \"\"\"\n",
        "        Update network parameters with a single mini-batch step of backpropagation and gradient descent.\n",
        "\n",
        "        Args:\n",
        "        - x_mini_batch: shape (B, N^0) where B is mini_batch_size.\n",
        "        - y_mini_batch: shape (B, N^last).\n",
        "        - learning_rate.\n",
        "        \"\"\"\n",
        "        grads_w, grads_b = self.backprop(x_mini_batch, y_mini_batch)\n",
        "\n",
        "        # Gradient descent step.\n",
        "        self.weights = [\n",
        "            w - learning_rate * grad_w\n",
        "            for w, grad_w in zip(self.weights, grads_w, strict=True)\n",
        "        ]\n",
        "        self.biases = [\n",
        "            b - learning_rate * grad_b\n",
        "            for b, grad_b in zip(self.biases, grads_b, strict=True)\n",
        "        ]\n",
        "\n",
        "    def backprop(\n",
        "        self, x: FloatNDArray, y: FloatNDArray\n",
        "    ) -> tuple[list[FloatNDArray], list[FloatNDArray]]:\n",
        "        \"\"\"\n",
        "        Backpropagation for a mini-batch (vectorized).\n",
        "\n",
        "        Args:\n",
        "        - x: input, shape (B, N^0)\n",
        "        - y: target label (one-hot encoded), shape (B, N^last)\n",
        "\n",
        "        Returns (grads_w, grads_b), where:\n",
        "        - grads_w: list of gradients over weights (shape (N^i, N^{i-1})), for each layer.\n",
        "        - grads_b: list of gradients over biases (shape (N^i)), for each layer.\n",
        "        \"\"\"\n",
        "        B, N0 = x.shape\n",
        "        assert N0 == self.sizes[0]\n",
        "\n",
        "        # Forward pass\n",
        "        activations = [x] # list to store all the activations, layer by layer\n",
        "        zs = [] # list to store all the z vectors, layer by layer\n",
        "        activation = x\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(activation, w.T) + b\n",
        "            zs.append(z)\n",
        "            activation = sigmoid(z)\n",
        "            activations.append(activation)\n",
        "\n",
        "        # Backward pass\n",
        "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
        "        grads_b = [np.sum(delta, axis=0)]\n",
        "        grads_w = [np.dot(delta.T, activations[-2])]\n",
        "\n",
        "        for l in range(2, self.num_layers + 1):\n",
        "            z = zs[-l]\n",
        "            sp = sigmoid_prime(z)\n",
        "            delta = np.dot(delta, self.weights[-l+1]) * sp\n",
        "            grads_b.append(np.sum(delta, axis=0))\n",
        "            grads_w.append(np.dot(delta.T, activations[-l-1]))\n",
        "\n",
        "        grads_w.reverse()\n",
        "        grads_b.reverse()\n",
        "        return grads_w, grads_b\n",
        "\n",
        "\n",
        "    def cost_derivative(self, a: FloatNDArray, y: FloatNDArray) -> FloatNDArray:\n",
        "        \"\"\"\n",
        "        Gradient of loss (MSE) over output activations.\n",
        "\n",
        "        Args:\n",
        "        - a: output activations, shape (B, N^last).\n",
        "        - y: target values (one-hot encoded labels), shape (B, N^last).\n",
        "\n",
        "        Returns gradients, shape (B, N^last).\n",
        "        \"\"\"\n",
        "        assert a.shape == y.shape, f\"Shape mismatch: {a.shape=} but {y.shape=}\"\n",
        "        B, N_last = a.shape\n",
        "        return (2 / (B * N_last)) * (a - y.astype(np.float64))\n",
        "\n",
        "    def evaluate(self, x_test_data: FloatNDArray, y_test_data: FloatNDArray) -> np.float64:\n",
        "        \"\"\"\n",
        "        Compute accuracy: the ratio of correct answers for test_data.\n",
        "\n",
        "        Args:\n",
        "        - x_test_data: shape (B, N^0).\n",
        "        - y_test_data: shape (B, N^last).\n",
        "        \"\"\"\n",
        "        predictions = np.argmax(self.feedforward(x_test_data), axis=1)\n",
        "        targets = np.argmax(y_test_data, axis=1)\n",
        "        return np.mean(predictions == targets)\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        training_data: tuple[FloatNDArray, FloatNDArray],\n",
        "        test_data: tuple[FloatNDArray, FloatNDArray] | None = None,\n",
        "        epochs: int = 2,\n",
        "        mini_batch_size: int = 100,\n",
        "        learning_rate: float = 1.0\n",
        "    ) -> np.float64:\n",
        "        x_train, y_train = training_data\n",
        "        progress_bar = tqdm(range(epochs), desc=\"Epoch\")\n",
        "        for epoch in progress_bar:\n",
        "            for i in range(x_train.shape[0] // mini_batch_size):\n",
        "                i_begin = i * mini_batch_size\n",
        "                i_end = (i + 1) * mini_batch_size\n",
        "                self.learning_step(x_train[i_begin:i_end], y_train[i_begin:i_end], learning_rate)\n",
        "            if test_data:\n",
        "                x_test, y_test = test_data\n",
        "                accuracy = self.evaluate(x_test, y_test)\n",
        "                progress_bar.set_postfix_str(f\"Test accuracy: {accuracy * 100:.2f} %\")\n",
        "\n",
        "        if test_data:\n",
        "            x_test, y_test = test_data\n",
        "            return self.evaluate(x_test, y_test)\n",
        "        else:\n",
        "            return np.float64(-1)\n",
        "\n",
        "model_results.evaluate_model(model_name=\"Baseline\", model_constructor=Network, n_trainings=3)\n",
        "model_results.display_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpZIY72SXywD"
      },
      "source": [
        "## Task 1: softmax & cross-entropy loss\n",
        "Use softmax instead of coordinate-wise sigmoid and use negative-log-loss instead of MSE. Test to see if this improves convergence.   \n",
        "\n",
        "Hints:\n",
        "* When implementing backprop it's easier to consider these two functions as a single block, skipping the computation of the gradient over the softmax values, and going directly to gradients over logits (last pre-activations).\n",
        "* Softmax is only used after the last layer; previous layers (and their grad computations) can be unchanged.\n",
        "* Remember to update the forward pass in both places.\n",
        "* Loss for a mini-batch is the mean of losses for each dataitem in it, by convention.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "77iGbiSDXywD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "outputId": "776664fa-15b5-4ffe-c2ba-b00a2ca7fcba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking 3 random trainings with with lr = 0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 10/10 [00:08<00:00,  1.15it/s, Test accuracy: 91.96 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:09<00:00,  1.07it/s, Test accuracy: 79.04 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:07<00:00,  1.38it/s, Test accuracy: 90.63 %]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7932d7d0ec60>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_f2c4b\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th id=\"T_f2c4b_level0_col0\" class=\"col_heading level0 col0\" >model</th>\n",
              "      <th id=\"T_f2c4b_level0_col1\" class=\"col_heading level0 col1\" >lr</th>\n",
              "      <th id=\"T_f2c4b_level0_col2\" class=\"col_heading level0 col2\" >accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td id=\"T_f2c4b_row0_col0\" class=\"data row0 col0\" >Baseline</td>\n",
              "      <td id=\"T_f2c4b_row0_col1\" class=\"data row0 col1\" >1e+01</td>\n",
              "      <td id=\"T_f2c4b_row0_col2\" class=\"data row0 col2\" >94.8% ± 0.1 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_f2c4b_row1_col0\" class=\"data row1 col0\" >Baseline</td>\n",
              "      <td id=\"T_f2c4b_row1_col1\" class=\"data row1 col1\" >1</td>\n",
              "      <td id=\"T_f2c4b_row1_col2\" class=\"data row1 col2\" >91.0% ± 0.1 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_f2c4b_row2_col0\" class=\"data row2 col0\" >SoftMax</td>\n",
              "      <td id=\"T_f2c4b_row2_col1\" class=\"data row2 col1\" >0.1</td>\n",
              "      <td id=\"T_f2c4b_row2_col2\" class=\"data row2 col2\" >87.2% ± 5.8 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_f2c4b_row3_col0\" class=\"data row3 col0\" >Baseline</td>\n",
              "      <td id=\"T_f2c4b_row3_col1\" class=\"data row3 col1\" >1e+02</td>\n",
              "      <td id=\"T_f2c4b_row3_col2\" class=\"data row3 col2\" >9.8% ± 0.6 p.p.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "class Task1(Network):\n",
        "    def __init__(self, sizes: Sequence[int]):\n",
        "        super().__init__(sizes=sizes)\n",
        "\n",
        "    def feedforward(self, x: FloatNDArray) -> FloatNDArray:\n",
        "        # This feedforward is for training and calculating the loss.\n",
        "        # The evaluate method in the parent class will use this feedforward,\n",
        "        # and then take the argmax, which is correct for evaluation.\n",
        "        g = x\n",
        "        for w, b in zip(self.weights, self.biases, strict=True):\n",
        "            # Shapes (B, N^{i-1}) @ (N^{i-1}, N^i) + (N^i,)  ==  (B, N^i)\n",
        "            # Apply sigmoid for hidden layers\n",
        "            if w is not self.weights[-1]:\n",
        "                g = sigmoid(g @ w.T + b)\n",
        "            else:\n",
        "                # Apply stable_softmax for the output layer\n",
        "                g = stable_softmax(g @ w.T + b)\n",
        "        return g\n",
        "\n",
        "    def backprop(\n",
        "        self, x: FloatNDArray, y: FloatNDArray\n",
        "    ) -> tuple[list[FloatNDArray], list[FloatNDArray]]:\n",
        "        B, N0 = x.shape\n",
        "        assert N0 == self.sizes[0]\n",
        "\n",
        "        # Forward pass\n",
        "        activations = [x] # list to store all the activations, layer by layer\n",
        "        zs = [] # list to store all the z vectors, layer by layer\n",
        "        activation = x\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(activation, w.T) + b\n",
        "            zs.append(z)\n",
        "            # Apply sigmoid for hidden layers and stable_softmax for the output layer\n",
        "            if w is not self.weights[-1]:\n",
        "                activation = sigmoid(z)\n",
        "            else:\n",
        "                activation = stable_softmax(z)\n",
        "            activations.append(activation)\n",
        "\n",
        "        # Backward pass (using the combined gradient for softmax and cross-entropy)\n",
        "        # The gradient of the cross-entropy loss with respect to the logits (zs[-1]) is simply (activations[-1] - y)\n",
        "        delta = activations[-1] - y\n",
        "\n",
        "        grads_b = [np.sum(delta, axis=0)]\n",
        "        grads_w = [np.dot(delta.T, activations[-2])]\n",
        "\n",
        "        for l in range(2, self.num_layers + 1):\n",
        "            z = zs[-l]\n",
        "            sp = sigmoid_prime(z) # Use sigmoid_prime for hidden layers\n",
        "            delta = np.dot(delta, self.weights[-l+1]) * sp\n",
        "            grads_b.append(np.sum(delta, axis=0))\n",
        "            grads_w.append(np.dot(delta.T, activations[-l-1]))\n",
        "\n",
        "        grads_w.reverse()\n",
        "        grads_b.reverse()\n",
        "\n",
        "\n",
        "        for grad_b, b in zip(grads_b, self.biases, strict=True):\n",
        "            assert grad_b.shape == b.shape, f\"Shape mismatch: {grad_b.shape=} but {b.shape=}\"\n",
        "        for grad_w, w in zip(grads_w, self.weights, strict=True):\n",
        "            assert grad_w.shape == w.shape, f\"Shape mismatch: {grad_w.shape=} but {w.shape=}\"\n",
        "\n",
        "        return grads_w, grads_b\n",
        "\n",
        "model_results.evaluate_model(model_name=\"SoftMax\", model_constructor=Task1, n_trainings=3, learning_rates=[0.1])\n",
        "model_results.display_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvIk4RxTXywD"
      },
      "source": [
        "## Task 2: L2-regularization and momentum\n",
        "Implement L2-regularization and add momentum to the SGD algorithm. Play with different amounts of regularization and momentum. See if this improves accuracy/convergence.  \n",
        "A few notes:\n",
        "* do not regularize the biases\n",
        "* you can see an example pseudocode here [pytorch.org/docs/stable/generated/torch.optim.SGD.html](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "s3-03midXywD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "f1d91b34-5ac2-4520-b1f5-095f9343173d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking 1 random trainings with with lr = 10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 10/10 [00:10<00:00,  1.04s/it, Test accuracy: 94.83 %]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7932d54c3b30>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_dd482\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th id=\"T_dd482_level0_col0\" class=\"col_heading level0 col0\" >model</th>\n",
              "      <th id=\"T_dd482_level0_col1\" class=\"col_heading level0 col1\" >lr</th>\n",
              "      <th id=\"T_dd482_level0_col2\" class=\"col_heading level0 col2\" >accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td id=\"T_dd482_row0_col0\" class=\"data row0 col0\" >L2&Momentum(l2_factor=1e-05,momentum=0.2)</td>\n",
              "      <td id=\"T_dd482_row0_col1\" class=\"data row0 col1\" >1e+01</td>\n",
              "      <td id=\"T_dd482_row0_col2\" class=\"data row0 col2\" >94.8% ± 0.0 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_dd482_row1_col0\" class=\"data row1 col0\" >Baseline</td>\n",
              "      <td id=\"T_dd482_row1_col1\" class=\"data row1 col1\" >1e+01</td>\n",
              "      <td id=\"T_dd482_row1_col2\" class=\"data row1 col2\" >94.8% ± 0.1 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_dd482_row2_col0\" class=\"data row2 col0\" >Baseline</td>\n",
              "      <td id=\"T_dd482_row2_col1\" class=\"data row2 col1\" >1</td>\n",
              "      <td id=\"T_dd482_row2_col2\" class=\"data row2 col2\" >91.0% ± 0.1 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_dd482_row3_col0\" class=\"data row3 col0\" >SoftMax</td>\n",
              "      <td id=\"T_dd482_row3_col1\" class=\"data row3 col1\" >0.1</td>\n",
              "      <td id=\"T_dd482_row3_col2\" class=\"data row3 col2\" >87.2% ± 5.8 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_dd482_row4_col0\" class=\"data row4 col0\" >Baseline</td>\n",
              "      <td id=\"T_dd482_row4_col1\" class=\"data row4 col1\" >1e+02</td>\n",
              "      <td id=\"T_dd482_row4_col2\" class=\"data row4 col2\" >9.8% ± 0.6 p.p.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "class Task2(Network):\n",
        "    def __init__(\n",
        "        self, sizes: Sequence[int], l2_factor: float = 1e-5, momentum: float = 0.2\n",
        "    ):\n",
        "        super().__init__(sizes=sizes)\n",
        "        self.l2_factor = l2_factor\n",
        "        self.momentum = momentum\n",
        "        # Initialize velocity for momentum\n",
        "        self.velocity_w = [np.zeros_like(w) for w in self.weights]\n",
        "        self.velocity_b = [np.zeros_like(b) for b in self.biases]\n",
        "\n",
        "    def learning_step(self, x_mini_batch: FloatNDArray, y_mini_batch: FloatNDArray, learning_rate: float) -> None:\n",
        "        \"\"\"\n",
        "        Update parameters with one mini-batch step of backprop and gradient descent (with momentum and L2-regularization).\n",
        "\n",
        "        Args:\n",
        "        - x_mini_batch: shape (B, N^0) where B is mini_batch_size.\n",
        "        - y_mini_batch: shape (B, N^last).\n",
        "        - learning_rate.\n",
        "        \"\"\"\n",
        "        grads_w, grads_b = self.backprop(x_mini_batch, y_mini_batch)\n",
        "\n",
        "        # Apply L2 regularization and momentum\n",
        "        for i in range(len(self.weights)):\n",
        "            # L2 regularization for weights (do not regularize biases)\n",
        "            grad_w_l2 = grads_w[i] + self.l2_factor * self.weights[i]\n",
        "\n",
        "            # Momentum for weights\n",
        "            self.velocity_w[i] = self.momentum * self.velocity_w[i] - learning_rate * grad_w_l2\n",
        "            self.weights[i] += self.velocity_w[i]\n",
        "\n",
        "            # Momentum for biases\n",
        "            self.velocity_b[i] = self.momentum * self.velocity_b[i] - learning_rate * grads_b[i]\n",
        "            self.biases[i] += self.velocity_b[i]\n",
        "\n",
        "model_results.evaluate_model(\n",
        "    model_name=f\"L2&Momentum\",\n",
        "    model_constructor=Task2,\n",
        "    learning_rates=[10.0],\n",
        "    n_trainings=1,\n",
        "    l2_factor=1e-5,\n",
        "    momentum=0.2\n",
        ")\n",
        "model_results.display_results()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nnBGG1xo0F34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "outputId": "862daf27-1785-4ed5-bf44-a329e028efa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking 3 random trainings with with lr = 2.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:   0%|          | 0/10 [00:00<?, ?it/s]/tmp/ipython-input-3898644277.py:2: RuntimeWarning: overflow encountered in exp\n",
            "  return 1.0 / (1.0 + np.exp(-z))\n",
            "Epoch: 100%|██████████| 10/10 [00:10<00:00,  1.06s/it, Test accuracy: 9.82 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:10<00:00,  1.07s/it, Test accuracy: 10.10 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:08<00:00,  1.16it/s, Test accuracy: 10.10 %]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7932d54c38c0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_3a663\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th id=\"T_3a663_level0_col0\" class=\"col_heading level0 col0\" >model</th>\n",
              "      <th id=\"T_3a663_level0_col1\" class=\"col_heading level0 col1\" >lr</th>\n",
              "      <th id=\"T_3a663_level0_col2\" class=\"col_heading level0 col2\" >accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td id=\"T_3a663_row0_col0\" class=\"data row0 col0\" >L2&Momentum(l2_factor=1e-05,momentum=0.2)</td>\n",
              "      <td id=\"T_3a663_row0_col1\" class=\"data row0 col1\" >1e+01</td>\n",
              "      <td id=\"T_3a663_row0_col2\" class=\"data row0 col2\" >94.8% ± 0.0 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_3a663_row1_col0\" class=\"data row1 col0\" >Baseline</td>\n",
              "      <td id=\"T_3a663_row1_col1\" class=\"data row1 col1\" >1e+01</td>\n",
              "      <td id=\"T_3a663_row1_col2\" class=\"data row1 col2\" >94.8% ± 0.1 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_3a663_row2_col0\" class=\"data row2 col0\" >Baseline</td>\n",
              "      <td id=\"T_3a663_row2_col1\" class=\"data row2 col1\" >1</td>\n",
              "      <td id=\"T_3a663_row2_col2\" class=\"data row2 col2\" >91.0% ± 0.1 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_3a663_row3_col0\" class=\"data row3 col0\" >SoftMax</td>\n",
              "      <td id=\"T_3a663_row3_col1\" class=\"data row3 col1\" >0.1</td>\n",
              "      <td id=\"T_3a663_row3_col2\" class=\"data row3 col2\" >87.2% ± 5.8 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_3a663_row4_col0\" class=\"data row4 col0\" >Softmax&L2&Momentum(l2_factor=1e-06,momentum=0.1)</td>\n",
              "      <td id=\"T_3a663_row4_col1\" class=\"data row4 col1\" >2</td>\n",
              "      <td id=\"T_3a663_row4_col2\" class=\"data row4 col2\" >10.0% ± 0.1 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_3a663_row5_col0\" class=\"data row5 col0\" >Baseline</td>\n",
              "      <td id=\"T_3a663_row5_col1\" class=\"data row5 col1\" >1e+02</td>\n",
              "      <td id=\"T_3a663_row5_col2\" class=\"data row5 col2\" >9.8% ± 0.6 p.p.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "class Task1And2(Task2, Task1):\n",
        "    # A somewhat hacky but short way to mix Task1 and Task2.\n",
        "    # You could also just replace the superclass of Task2 to be Task1.\n",
        "    pass\n",
        "\n",
        "model_results.evaluate_model(\n",
        "    model_name=f\"Softmax&L2&Momentum\",\n",
        "    model_constructor=Task1And2,\n",
        "    learning_rates=[2.0],\n",
        "    n_trainings=3,\n",
        "    l2_factor=1e-6,\n",
        "    momentum=0.1\n",
        ")\n",
        "model_results.display_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6nLauKUXywE"
      },
      "source": [
        "## Task 3 (optional)\n",
        "Implement more variations of SGD:\n",
        "* AdamW (probably the most popular choice) or Adagrad,\n",
        "* dropout\n",
        "* some simple data augmentations (e.g. tiny rotations/shifts etc.).\n",
        "\n",
        "Again, test to see how these changes improve accuracy/convergence.  \n",
        "\n",
        "Quick reminders:\n",
        "* for AdamW, check the official [PyTorch documentation](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html)'s pseudocode or the original paper: [Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101).\n",
        "* for AdaGrad, check the Appendix of this notebook.\n",
        "* for dropout: during training only, zero-out each activation in the considered layer with probability $p$, and multiplying other activations by $\\frac{1}{1-p}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8X3hRIizXywE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "3f3660bf-fa5c-4c46-c1e9-071fb4f6b8c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking 3 random trainings with with lr = 0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 10/10 [00:10<00:00,  1.07s/it, Test accuracy: 81.24 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:18<00:00,  1.87s/it, Test accuracy: 90.67 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:10<00:00,  1.07s/it, Test accuracy: 88.60 %]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking 3 random trainings with with lr = 0.05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 10/10 [00:10<00:00,  1.07s/it, Test accuracy: 93.59 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:12<00:00,  1.28s/it, Test accuracy: 94.58 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:08<00:00,  1.18it/s, Test accuracy: 94.09 %]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking 3 random trainings with with lr = 0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 10/10 [00:10<00:00,  1.07s/it, Test accuracy: 95.52 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:11<00:00,  1.11s/it, Test accuracy: 95.50 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:10<00:00,  1.08s/it, Test accuracy: 95.56 %]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7932f2a6b560>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_0d890\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th id=\"T_0d890_level0_col0\" class=\"col_heading level0 col0\" >model</th>\n",
              "      <th id=\"T_0d890_level0_col1\" class=\"col_heading level0 col1\" >lr</th>\n",
              "      <th id=\"T_0d890_level0_col2\" class=\"col_heading level0 col2\" >accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td id=\"T_0d890_row0_col0\" class=\"data row0 col0\" >Softmax&L2&Momentum&Dropout(l2_factor=1e-06,momentum=0.1,dropout=0.2)</td>\n",
              "      <td id=\"T_0d890_row0_col1\" class=\"data row0 col1\" >0.01</td>\n",
              "      <td id=\"T_0d890_row0_col2\" class=\"data row0 col2\" >95.5% ± 0.0 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_0d890_row1_col0\" class=\"data row1 col0\" >L2&Momentum(l2_factor=1e-05,momentum=0.2)</td>\n",
              "      <td id=\"T_0d890_row1_col1\" class=\"data row1 col1\" >1e+01</td>\n",
              "      <td id=\"T_0d890_row1_col2\" class=\"data row1 col2\" >94.8% ± 0.0 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_0d890_row2_col0\" class=\"data row2 col0\" >Baseline</td>\n",
              "      <td id=\"T_0d890_row2_col1\" class=\"data row2 col1\" >1e+01</td>\n",
              "      <td id=\"T_0d890_row2_col2\" class=\"data row2 col2\" >94.8% ± 0.1 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_0d890_row3_col0\" class=\"data row3 col0\" >Softmax&L2&Momentum&Dropout(l2_factor=1e-06,momentum=0.1,dropout=0.2)</td>\n",
              "      <td id=\"T_0d890_row3_col1\" class=\"data row3 col1\" >0.05</td>\n",
              "      <td id=\"T_0d890_row3_col2\" class=\"data row3 col2\" >94.1% ± 0.4 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_0d890_row4_col0\" class=\"data row4 col0\" >Baseline</td>\n",
              "      <td id=\"T_0d890_row4_col1\" class=\"data row4 col1\" >1</td>\n",
              "      <td id=\"T_0d890_row4_col2\" class=\"data row4 col2\" >91.0% ± 0.1 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_0d890_row5_col0\" class=\"data row5 col0\" >SoftMax</td>\n",
              "      <td id=\"T_0d890_row5_col1\" class=\"data row5 col1\" >0.1</td>\n",
              "      <td id=\"T_0d890_row5_col2\" class=\"data row5 col2\" >87.2% ± 5.8 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_0d890_row6_col0\" class=\"data row6 col0\" >Softmax&L2&Momentum&Dropout(l2_factor=1e-06,momentum=0.1,dropout=0.2)</td>\n",
              "      <td id=\"T_0d890_row6_col1\" class=\"data row6 col1\" >0.1</td>\n",
              "      <td id=\"T_0d890_row6_col2\" class=\"data row6 col2\" >86.8% ± 4.0 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_0d890_row7_col0\" class=\"data row7 col0\" >Softmax&L2&Momentum&Dropout(l2_factor=1e-06,momentum=0.1,dropout=0.2)</td>\n",
              "      <td id=\"T_0d890_row7_col1\" class=\"data row7 col1\" >2</td>\n",
              "      <td id=\"T_0d890_row7_col2\" class=\"data row7 col2\" >12.6% ± 3.0 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_0d890_row8_col0\" class=\"data row8 col0\" >Softmax&L2&Momentum(l2_factor=1e-06,momentum=0.1)</td>\n",
              "      <td id=\"T_0d890_row8_col1\" class=\"data row8 col1\" >2</td>\n",
              "      <td id=\"T_0d890_row8_col2\" class=\"data row8 col2\" >10.0% ± 0.1 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_0d890_row9_col0\" class=\"data row9 col0\" >Baseline</td>\n",
              "      <td id=\"T_0d890_row9_col1\" class=\"data row9 col1\" >1e+02</td>\n",
              "      <td id=\"T_0d890_row9_col2\" class=\"data row9 col2\" >9.8% ± 0.6 p.p.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "class Task3(Task1And2):\n",
        "    def __init__(self, sizes: Sequence[int],  l2_factor: float = 1e-5, momentum: float = 0.2, dropout: float = 0.0):\n",
        "        super().__init__(sizes=sizes, l2_factor=l2_factor, momentum=momentum)\n",
        "        self.dropout = dropout\n",
        "        self.dropout_masks = [] # To store masks for backprop\n",
        "\n",
        "    # Removed the incorrect feedforward override. The feedforward from Task1And2 (which is Task1's) will be used for inference.\n",
        "\n",
        "    def backprop(\n",
        "        self, x: FloatNDArray, y: FloatNDArray\n",
        "    ) -> tuple[list[FloatNDArray], list[FloatNDArray]]:\n",
        "        B, N0 = x.shape\n",
        "        assert N0 == self.sizes[0]\n",
        "\n",
        "        self.dropout_masks = [] # Clear masks for each mini-batch\n",
        "\n",
        "        # Forward pass with dropout\n",
        "        activations = [x] # list to store all the activations, layer by layer\n",
        "        zs = [] # list to store all the z vectors, layer by layer\n",
        "        activation = x\n",
        "        for i, (b, w) in enumerate(zip(self.biases, self.weights)):\n",
        "            z = np.dot(activation, w.T) + b\n",
        "            zs.append(z)\n",
        "            if i < self.num_layers - 1: # Apply dropout to hidden layers (all except the last one)\n",
        "                activation = sigmoid(z)\n",
        "                if self.dropout > 0:\n",
        "                    mask = np.random.binomial(1, 1 - self.dropout, size=activation.shape)\n",
        "                    activation = activation * mask / (1 - self.dropout) # Inverted dropout scaling\n",
        "                    self.dropout_masks.append(mask)\n",
        "                else:\n",
        "                    # Append dummy mask if no dropout to keep list length consistent for backprop\n",
        "                    self.dropout_masks.append(np.ones_like(activation))\n",
        "            else: # Output layer\n",
        "                activation = stable_softmax(z)\n",
        "            activations.append(activation)\n",
        "\n",
        "        # Backward pass (using the combined gradient for softmax and cross-entropy)\n",
        "        delta = activations[-1] - y\n",
        "\n",
        "        grads_b = [np.sum(delta, axis=0)]\n",
        "        grads_w = [np.dot(delta.T, activations[-2])]\n",
        "\n",
        "        # Loop from the second to last layer back to the first hidden layer\n",
        "        for l in range(2, self.num_layers + 1):\n",
        "            z = zs[-l]\n",
        "            sp = sigmoid_prime(z)\n",
        "\n",
        "            # Propagate delta backward, through the weights of the *next* layer\n",
        "            delta = np.dot(delta, self.weights[-l+1]) * sp\n",
        "\n",
        "            # Apply dropout mask to delta for hidden layers if dropout was active\n",
        "            # The dropout_masks are indexed from 0 for activations[1] (first hidden layer)\n",
        "            # and self.num_layers - 2 for activations[self.num_layers - 1] (last hidden layer)\n",
        "            # The index for activations[-l] is (self.num_layers - l)\n",
        "            if self.dropout > 0 and (self.num_layers - l) >= 0 and (self.num_layers - l) < len(self.dropout_masks):\n",
        "                delta = delta * self.dropout_masks[self.num_layers - l] / (1 - self.dropout) # Re-apply inverted dropout scaling\n",
        "\n",
        "            grads_b.append(np.sum(delta, axis=0))\n",
        "            grads_w.append(np.dot(delta.T, activations[-l-1]))\n",
        "\n",
        "        grads_w.reverse()\n",
        "        grads_b.reverse()\n",
        "\n",
        "        for grad_b, b in zip(grads_b, self.biases, strict=True):\n",
        "            assert grad_b.shape == b.shape, f\"Shape mismatch: {grad_b.shape=} but {b.shape=}\"\n",
        "        for grad_w, w in zip(grads_w, self.weights, strict=True):\n",
        "            assert grad_w.shape == w.shape, f\"Shape mismatch: {grad_w.shape=} but {w.shape=}\"\n",
        "\n",
        "        return grads_w, grads_b\n",
        "\n",
        "model_results.evaluate_model(\n",
        "    model_name=f\"Softmax&L2&Momentum&Dropout\",\n",
        "    model_constructor=Task3,\n",
        "    learning_rates=[0.1, 0.05, 0.01], # Test lower learning rates\n",
        "    n_trainings=3,\n",
        "    l2_factor=1e-6,\n",
        "    momentum=0.1,\n",
        "    dropout=0.2\n",
        ")\n",
        "model_results.display_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HCbwiW2XywE"
      },
      "source": [
        "## Task 4\n",
        "Try adding extra layers to the network. Again, test how the changes you introduced affect accuracy/convergence and what learning rates work.\n",
        "\n",
        "As a start, you can try this slightly larger architecture: [784,100,30,10]  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "92OPX1uCXywE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9abdeabc-704f-4995-827c-e4cdcab4c313"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking 3 random trainings with with lr = 0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 10/10 [00:23<00:00,  2.32s/it, Test accuracy: 9.74 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:21<00:00,  2.13s/it, Test accuracy: 96.37 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:23<00:00,  2.33s/it, Test accuracy: 9.74 %]\n"
          ]
        }
      ],
      "source": [
        "model_results.evaluate_model(\n",
        "    model_name=f\"ExtraLayers\",\n",
        "    model_constructor=Task1And2,\n",
        "    layers=[784,100,30,10],\n",
        "    learning_rates=[0.1],\n",
        "    n_trainings=3,\n",
        "    l2_factor=1e-6,\n",
        "    momentum=0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_results.display_results()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "id": "fj1oYDL13W7t",
        "outputId": "dcad7ec5-b250-4158-8d65-85233bd05435"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7932d54f11c0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_4a62f\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th id=\"T_4a62f_level0_col0\" class=\"col_heading level0 col0\" >model</th>\n",
              "      <th id=\"T_4a62f_level0_col1\" class=\"col_heading level0 col1\" >lr</th>\n",
              "      <th id=\"T_4a62f_level0_col2\" class=\"col_heading level0 col2\" >accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td id=\"T_4a62f_row0_col0\" class=\"data row0 col0\" >Softmax&L2&Momentum&Dropout(l2_factor=1e-06,momentum=0.1,dropout=0.2)</td>\n",
              "      <td id=\"T_4a62f_row0_col1\" class=\"data row0 col1\" >0.01</td>\n",
              "      <td id=\"T_4a62f_row0_col2\" class=\"data row0 col2\" >95.5% ± 0.0 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_4a62f_row1_col0\" class=\"data row1 col0\" >L2&Momentum(l2_factor=1e-05,momentum=0.2)</td>\n",
              "      <td id=\"T_4a62f_row1_col1\" class=\"data row1 col1\" >1e+01</td>\n",
              "      <td id=\"T_4a62f_row1_col2\" class=\"data row1 col2\" >94.8% ± 0.0 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_4a62f_row2_col0\" class=\"data row2 col0\" >Baseline</td>\n",
              "      <td id=\"T_4a62f_row2_col1\" class=\"data row2 col1\" >1e+01</td>\n",
              "      <td id=\"T_4a62f_row2_col2\" class=\"data row2 col2\" >94.8% ± 0.1 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_4a62f_row3_col0\" class=\"data row3 col0\" >Softmax&L2&Momentum&Dropout(l2_factor=1e-06,momentum=0.1,dropout=0.2)</td>\n",
              "      <td id=\"T_4a62f_row3_col1\" class=\"data row3 col1\" >0.05</td>\n",
              "      <td id=\"T_4a62f_row3_col2\" class=\"data row3 col2\" >94.1% ± 0.4 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_4a62f_row4_col0\" class=\"data row4 col0\" >Baseline</td>\n",
              "      <td id=\"T_4a62f_row4_col1\" class=\"data row4 col1\" >1</td>\n",
              "      <td id=\"T_4a62f_row4_col2\" class=\"data row4 col2\" >91.0% ± 0.1 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_4a62f_row5_col0\" class=\"data row5 col0\" >SoftMax</td>\n",
              "      <td id=\"T_4a62f_row5_col1\" class=\"data row5 col1\" >0.1</td>\n",
              "      <td id=\"T_4a62f_row5_col2\" class=\"data row5 col2\" >87.2% ± 5.8 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_4a62f_row6_col0\" class=\"data row6 col0\" >Softmax&L2&Momentum&Dropout(l2_factor=1e-06,momentum=0.1,dropout=0.2)</td>\n",
              "      <td id=\"T_4a62f_row6_col1\" class=\"data row6 col1\" >0.1</td>\n",
              "      <td id=\"T_4a62f_row6_col2\" class=\"data row6 col2\" >86.8% ± 4.0 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_4a62f_row7_col0\" class=\"data row7 col0\" >ExtraLayers[784,100,30,10](l2_factor=1e-06,momentum=0.1)</td>\n",
              "      <td id=\"T_4a62f_row7_col1\" class=\"data row7 col1\" >0.1</td>\n",
              "      <td id=\"T_4a62f_row7_col2\" class=\"data row7 col2\" >38.6% ± 40.8 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_4a62f_row8_col0\" class=\"data row8 col0\" >Softmax&L2&Momentum&Dropout(l2_factor=1e-06,momentum=0.1,dropout=0.2)</td>\n",
              "      <td id=\"T_4a62f_row8_col1\" class=\"data row8 col1\" >2</td>\n",
              "      <td id=\"T_4a62f_row8_col2\" class=\"data row8 col2\" >12.6% ± 3.0 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_4a62f_row9_col0\" class=\"data row9 col0\" >Softmax&L2&Momentum(l2_factor=1e-06,momentum=0.1)</td>\n",
              "      <td id=\"T_4a62f_row9_col1\" class=\"data row9 col1\" >2</td>\n",
              "      <td id=\"T_4a62f_row9_col2\" class=\"data row9 col2\" >10.0% ± 0.1 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_4a62f_row10_col0\" class=\"data row10 col0\" >Baseline</td>\n",
              "      <td id=\"T_4a62f_row10_col1\" class=\"data row10 col1\" >1e+02</td>\n",
              "      <td id=\"T_4a62f_row10_col2\" class=\"data row10 col2\" >9.8% ± 0.6 p.p.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xl3A1WSXywE"
      },
      "source": [
        "# Appendix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRX8ith-XywF"
      },
      "source": [
        "## Adagrad (simplified version)\n",
        "\n",
        "Let $p_1, \\ldots, p_n$ be all parameters in our model (weights and biases).  \n",
        "For parameter $p_i$ we maintain a variable $G_i$ (can be set to $0$ initially).\n",
        "Let $\\mathcal{L}$ be our loss without L2.   \n",
        "We update $G_i$ and $p_i$ each training step as follows:  \n",
        "$$\n",
        "G_i = G_i +  \\left(\\frac{\\partial \\mathcal{L}}{\\partial p_i}\\right)^2\\\\\n",
        "p_i = p_i - \\frac{\\eta}{\\sqrt{\\left(G_i + \\epsilon\\right)}}\\frac{\\partial \\mathcal{L}}{\\partial p_i}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cfBEYR4rYXmy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "WWFnNWF8fWJB"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}