{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Peephole LSTM\n",
        "\n",
        "Given an implementation of an LSTM module:\n",
        "\\begin{align}\n",
        "i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\\\\n",
        "f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\\\\n",
        "g_t = tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{go}) \\\\\n",
        "o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\\\\n",
        "c_t = f_t \\odot c_{t-1} + i_t \\odot g_t \\\\\n",
        "h_t = o_t \\odot tanh(c_t)\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "Your task is to modify the implementaiton to add [peephole connections](https://en.wikipedia.org/wiki/Long_short-term_memory#Peephole_LSTM) according to:\n",
        "\n",
        "\\begin{align}\n",
        "i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{ci} c_{t-1} + b_{ci}) \\\\\n",
        "f_t = \\sigma(W_{if} x_t + b_{if} + W_{cf} c_{t-1} + b_{cf}) \\\\\n",
        "o_t = \\sigma(W_{io} x_t + b_{io} + W_{co} c_{t-1} + b_{co}) \\\\\n",
        "c_t = f_t \\odot c_{t-1} + i_t \\odot tanh(W_{ic} x_t + b_{ic}) \\\\\n",
        "h_t = o_t \\odot c_t\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "dvCG1V_63yLu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nh3vt6p_3tUi"
      },
      "outputs": [],
      "source": [
        "import typing\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size: int, hidden_size: int, batch_first: bool):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.batch_first = batch_first\n",
        "\n",
        "        #input gate\n",
        "        self.W_ii = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
        "        self.W_hi = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        self.b_ii = nn.Parameter(torch.Tensor(hidden_size))\n",
        "        self.b_hi = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "        #forget gate\n",
        "        self.W_if = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
        "        self.W_hf = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        self.b_if = nn.Parameter(torch.Tensor(hidden_size))\n",
        "        self.b_hf = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "        #output gate c_t\n",
        "        self.W_ig = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
        "        self.W_hg = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        self.b_ig = nn.Parameter(torch.Tensor(hidden_size))\n",
        "        self.b_hg = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "        #output gate h_t\n",
        "        self.W_io = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
        "        self.W_ho = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        self.b_io = nn.Parameter(torch.Tensor(hidden_size))\n",
        "        self.b_ho = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "        self._init_parameters()\n",
        "\n",
        "    def _init_parameters(self):\n",
        "        for param in self.parameters():\n",
        "            torch.nn.init.normal_(param)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, hx: typing.Optional[typing.Tuple[torch.Tensor, torch.Tensor]] = None) -> typing.Tuple[torch.Tensor, typing.Tuple[torch.Tensor, torch.Tensor]]:\n",
        "\n",
        "        if not self.batch_first:\n",
        "            x = x.permute(1,0,2).contiguous()\n",
        "\n",
        "        batch_size = x.size(0)\n",
        "        sequence_length = x.size(1)\n",
        "\n",
        "        if hx is None:\n",
        "            h_t, c_t = (\n",
        "                torch.zeros(batch_size, self.hidden_size).to(x.device),\n",
        "                torch.zeros(batch_size, self.hidden_size).to(x.device),\n",
        "            )\n",
        "        else:\n",
        "            h_t, c_t = hx\n",
        "\n",
        "        output = []\n",
        "\n",
        "        for t in range(sequence_length):\n",
        "            x_t = x[:, t, :]\n",
        "            # input gate\n",
        "            i_t = torch.sigmoid(x_t @ self.W_ii + self.b_ii + h_t @ self.W_hi + self.b_hi)\n",
        "            # forget gate\n",
        "            f_t = torch.sigmoid(x_t @ self.W_if + self.b_if + h_t @ self.W_hf + self.b_hf)\n",
        "            # output gate\n",
        "            g_t = torch.tanh(x_t @ self.W_ig + self.b_ig + h_t @ self.W_hg + self.b_hg)\n",
        "            o_t = torch.sigmoid(x_t @ self.W_io + self.b_io + h_t @ self.W_ho + self.b_ho)\n",
        "\n",
        "            # output\n",
        "            c_t = f_t * c_t + i_t * g_t\n",
        "            h_t = o_t * torch.tanh(c_t)\n",
        "\n",
        "            output.append(h_t.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat(output, dim=0)\n",
        "\n",
        "        if not self.batch_first:\n",
        "            output = output.permute(1,0,2).contiguous()\n",
        "\n",
        "        return output, (h_t, c_t)\n"
      ],
      "metadata": {
        "id": "zm4oIViI3xzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "a = torch.randn((5,10, 3))\n",
        "lstm = LSTM(3, 7, True)\n",
        "print(lstm(a)[0].size(), lstm(a)[1][0].size(), lstm(a)[1][1].size())\n",
        "print(lstm(a))"
      ],
      "metadata": {
        "id": "B6AeaQFWJUJ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dff9d4c-603f-4dda-fadf-6bdc3976ccfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 5, 7]) torch.Size([5, 7]) torch.Size([5, 7])\n",
            "(tensor([[[ 6.2072e-01, -7.7352e-02,  5.6017e-01, -5.6047e-01,  3.5405e-04,\n",
            "           5.4798e-03,  3.5325e-01],\n",
            "         [ 7.1540e-01, -5.3192e-01,  9.5980e-02, -1.9399e-01,  1.1296e-02,\n",
            "           4.8099e-02, -4.7105e-02],\n",
            "         [ 6.1332e-01, -1.1772e-05,  5.9567e-01, -5.4451e-01,  1.6074e-04,\n",
            "           1.0339e-02,  5.2595e-01],\n",
            "         [ 3.0238e-01, -1.4361e-01,  8.4637e-02, -6.3363e-01,  4.3879e-04,\n",
            "          -9.2375e-02,  5.3165e-01],\n",
            "         [ 6.0016e-01, -9.1497e-05,  4.1435e-01, -2.1903e-01,  3.6609e-03,\n",
            "           2.7239e-02,  5.3316e-02]],\n",
            "\n",
            "        [[ 5.5158e-01, -1.1870e-02,  6.4174e-01, -1.0644e-01,  1.1541e-02,\n",
            "           1.1945e-01, -1.5863e-01],\n",
            "         [ 6.4038e-01, -7.3400e-02,  3.9188e-01, -4.2518e-01,  1.0513e-02,\n",
            "           4.5897e-02, -3.7773e-01],\n",
            "         [ 7.0326e-01, -6.1242e-04,  8.7785e-01, -6.8674e-01,  1.2353e-03,\n",
            "           2.7056e-02,  1.7479e-01],\n",
            "         [ 5.2825e-01, -1.5968e-03,  5.0265e-01, -4.6220e-01,  8.8791e-03,\n",
            "          -3.9077e-02,  4.6713e-01],\n",
            "         [ 6.3304e-01, -2.0938e-03,  7.2397e-01, -8.8802e-02,  6.7019e-03,\n",
            "           9.0267e-02, -3.4889e-01]],\n",
            "\n",
            "        [[ 7.9752e-01, -1.9431e-02,  5.0804e-01, -5.6571e-01,  4.7665e-05,\n",
            "           5.4940e-03, -8.4507e-02],\n",
            "         [ 6.9102e-01, -4.4645e-02,  2.3536e-01, -2.6317e-01,  1.5092e-02,\n",
            "           2.5829e-01, -2.3755e-01],\n",
            "         [ 6.4204e-01,  6.1721e-01,  7.7259e-01, -2.5114e-01,  1.6237e-02,\n",
            "          -1.0327e-01, -2.8043e-02],\n",
            "         [ 7.5423e-01, -6.3264e-02,  3.0276e-01, -2.4104e-01,  1.0912e-02,\n",
            "           1.2328e-02,  1.5663e-02],\n",
            "         [ 6.4298e-01,  6.7419e-01,  3.8288e-01,  1.3673e-01,  9.7270e-03,\n",
            "           2.7361e-01, -2.3666e-01]],\n",
            "\n",
            "        [[ 5.6152e-01, -2.8952e-03,  9.1750e-01, -7.1508e-01,  1.5369e-03,\n",
            "           2.8752e-02, -2.2981e-01],\n",
            "         [ 6.8171e-01, -4.7384e-02,  3.7066e-01, -2.1623e-01,  1.4963e-02,\n",
            "           2.6614e-01, -2.1687e-01],\n",
            "         [ 7.5715e-01,  3.4382e-01,  2.9007e-02,  1.1667e-02,  1.2793e-02,\n",
            "          -2.8848e-02, -5.8541e-03],\n",
            "         [ 9.4468e-01, -4.9082e-01,  6.5809e-01, -5.2392e-01,  2.4286e-04,\n",
            "           3.5660e-03,  4.5676e-02],\n",
            "         [ 7.4947e-01,  4.1291e-01,  7.8462e-01, -2.7983e-01,  1.0431e-03,\n",
            "           1.1284e-01, -1.8588e-01]],\n",
            "\n",
            "        [[ 6.6956e-01,  5.1983e-03,  4.8729e-01, -2.9663e-01,  4.5998e-03,\n",
            "           9.4431e-02, -3.1246e-01],\n",
            "         [ 6.4600e-01, -2.2212e-02,  6.0887e-01, -4.4284e-01,  1.5372e-02,\n",
            "           2.2399e-01, -4.5185e-01],\n",
            "         [ 8.4391e-01,  5.8274e-02,  4.3614e-01, -5.7180e-01,  6.6496e-03,\n",
            "          -1.0727e-01, -2.8017e-01],\n",
            "         [ 6.0281e-01, -4.3938e-02,  7.5523e-01, -5.5071e-01,  9.8586e-03,\n",
            "           8.6006e-02, -1.3197e-01],\n",
            "         [ 7.1893e-01,  6.2771e-01,  6.3621e-01,  2.8508e-02,  6.3245e-03,\n",
            "           2.5510e-01, -9.0510e-02]],\n",
            "\n",
            "        [[ 8.9070e-01, -4.5537e-01,  6.9151e-01, -5.8216e-01,  8.5808e-05,\n",
            "          -1.7484e-02, -1.6641e-01],\n",
            "         [ 7.6203e-01, -6.7927e-01,  4.8472e-02, -1.5304e-01,  1.3580e-03,\n",
            "           2.8377e-02, -8.0621e-03],\n",
            "         [ 7.2530e-01,  1.9701e-04,  6.4608e-01, -6.1042e-01,  2.6345e-03,\n",
            "          -4.1127e-03, -2.3357e-01],\n",
            "         [ 6.9349e-01, -1.8971e-02,  6.0225e-01, -3.0007e-01,  9.3045e-03,\n",
            "           2.4283e-01, -2.5774e-01],\n",
            "         [ 8.7703e-01,  4.1499e-01,  8.3987e-01, -4.6211e-01,  1.1046e-03,\n",
            "          -8.0159e-02, -8.4566e-02]],\n",
            "\n",
            "        [[ 6.5313e-01, -2.1047e-02,  3.5332e-01, -1.5911e-01,  3.8267e-03,\n",
            "          -4.1330e-02, -4.4606e-01],\n",
            "         [ 2.4666e-01, -3.0298e-03,  5.6387e-01, -1.7773e-01,  6.2917e-02,\n",
            "           2.0532e-01, -9.2768e-01],\n",
            "         [ 6.3189e-01,  1.2002e-01,  6.8673e-01, -3.9925e-01,  2.5041e-02,\n",
            "           6.2102e-02, -2.9426e-01],\n",
            "         [ 7.3693e-01, -2.2364e-02,  3.5856e-01, -3.6245e-01,  1.4355e-03,\n",
            "           7.7637e-02, -2.0268e-01],\n",
            "         [ 7.4127e-01,  6.5313e-01,  5.6145e-01, -1.8699e-01,  6.5541e-03,\n",
            "           6.9503e-02, -1.3809e-02]],\n",
            "\n",
            "        [[ 5.0800e-01, -5.6081e-05,  7.3610e-01, -6.3053e-01,  5.5730e-04,\n",
            "           8.0793e-03, -8.4990e-01],\n",
            "         [ 7.1438e-01,  2.5835e-01,  7.5456e-02, -3.5945e-01,  2.1063e-03,\n",
            "           1.6050e-01, -3.3942e-01],\n",
            "         [ 7.4654e-01, -3.1429e-02,  3.1602e-01, -3.9657e-01,  2.5840e-03,\n",
            "           4.1930e-02, -1.0682e-01],\n",
            "         [ 8.6267e-01,  1.5939e-01,  9.2005e-01, -6.6349e-01,  2.3380e-03,\n",
            "          -5.9504e-02, -3.0987e-01],\n",
            "         [ 6.9342e-01,  4.6951e-01,  8.3950e-01, -4.3182e-01,  8.1378e-03,\n",
            "           1.7351e-01, -1.9616e-01]],\n",
            "\n",
            "        [[ 7.4382e-01, -9.0354e-02,  4.0048e-02, -3.9025e-01,  1.8349e-04,\n",
            "           2.3589e-02, -1.1082e-01],\n",
            "         [ 7.2501e-01, -1.8047e-01,  2.3816e-01, -2.6741e-01,  9.0752e-03,\n",
            "           2.2361e-01, -5.3471e-02],\n",
            "         [ 7.1792e-01, -1.1871e-02,  3.5496e-01, -2.2188e-01,  1.0442e-02,\n",
            "           1.0401e-01, -1.4622e-01],\n",
            "         [ 7.1947e-01,  8.4102e-03,  5.9660e-01, -4.4864e-01,  4.0004e-03,\n",
            "          -2.6064e-01, -2.6324e-01],\n",
            "         [ 7.0810e-01,  4.9448e-01,  5.1478e-01, -2.8581e-02,  1.7034e-02,\n",
            "           2.8706e-01, -8.5273e-02]],\n",
            "\n",
            "        [[ 3.2567e-01,  1.2666e-03,  2.3614e-01, -1.4826e-01,  3.9025e-02,\n",
            "           2.9723e-01, -4.2845e-01],\n",
            "         [ 2.2068e-01, -3.5385e-05,  1.9582e-01, -5.2710e-02,  2.2287e-01,\n",
            "           6.0494e-01, -5.0587e-01],\n",
            "         [ 6.5139e-01, -2.3107e-04,  3.1102e-01, -1.4189e-01,  1.2394e-02,\n",
            "           1.6384e-01, -4.0079e-01],\n",
            "         [ 7.4339e-01, -3.6768e-02,  4.5767e-01, -5.0273e-01,  1.3255e-03,\n",
            "          -3.0857e-03, -2.3662e-01],\n",
            "         [ 6.4082e-01,  7.2440e-01,  6.4670e-01,  2.2681e-01,  1.6503e-02,\n",
            "           4.1172e-01, -1.6714e-01]]], grad_fn=<CatBackward0>), (tensor([[ 3.2567e-01,  1.2666e-03,  2.3614e-01, -1.4826e-01,  3.9025e-02,\n",
            "          2.9723e-01, -4.2845e-01],\n",
            "        [ 2.2068e-01, -3.5385e-05,  1.9582e-01, -5.2710e-02,  2.2287e-01,\n",
            "          6.0494e-01, -5.0587e-01],\n",
            "        [ 6.5139e-01, -2.3107e-04,  3.1102e-01, -1.4189e-01,  1.2394e-02,\n",
            "          1.6384e-01, -4.0079e-01],\n",
            "        [ 7.4339e-01, -3.6768e-02,  4.5767e-01, -5.0273e-01,  1.3255e-03,\n",
            "         -3.0857e-03, -2.3662e-01],\n",
            "        [ 6.4082e-01,  7.2440e-01,  6.4670e-01,  2.2681e-01,  1.6503e-02,\n",
            "          4.1172e-01, -1.6714e-01]], grad_fn=<MulBackward0>), tensor([[ 0.6466,  0.0178,  0.3329, -0.8764,  0.1602,  0.4033, -3.2700],\n",
            "        [ 0.8438, -0.0235,  0.8996, -0.6696,  0.8664,  0.8163, -5.9269],\n",
            "        [ 0.9337, -0.0277,  1.4046, -0.6232,  0.4201,  0.3336, -4.0720],\n",
            "        [ 0.9648, -0.0655,  1.4439, -1.5754,  0.1540, -0.0300, -2.7505],\n",
            "        [ 0.8024,  0.9421,  0.9388,  0.7231,  0.1783,  0.5671, -4.5616]],\n",
            "       grad_fn=<AddBackward0>)))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMPiphole(nn.Module):\n",
        "    def __init__(self, input_size: int, hidden_size: int, batch_first: bool):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.batch_first = batch_first\n",
        "\n",
        "        #input gate\n",
        "        self.W_ii = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
        "        self.W_ci = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        self.b_ii = nn.Parameter(torch.Tensor(hidden_size))\n",
        "        self.b_ci = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "        #forget gate\n",
        "        self.W_if = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
        "        self.W_cf = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        self.b_if = nn.Parameter(torch.Tensor(hidden_size))\n",
        "        self.b_cf = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "        #output gate c_t\n",
        "        self.W_ic = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
        "        self.b_ic = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "\n",
        "        #output gate h_t\n",
        "        self.W_io = nn.Parameter(torch.Tensor(input_size, hidden_size))\n",
        "        self.W_co = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        self.b_io = nn.Parameter(torch.Tensor(hidden_size))\n",
        "        self.b_co = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "        self._init_parameters()\n",
        "\n",
        "    def _init_parameters(self):\n",
        "        for param in self.parameters():\n",
        "            torch.nn.init.normal_(param)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, hx: typing.Optional[typing.Tuple[torch.Tensor, torch.Tensor]] = None) -> typing.Tuple[torch.Tensor, typing.Tuple[torch.Tensor, torch.Tensor]]:\n",
        "        #################################\n",
        "        # TODO: Implement forward pass  #\n",
        "        #################################\n",
        "\n",
        "        if not self.batch_first:\n",
        "            x = x.permute(1,0,2).contiguous()\n",
        "\n",
        "        batch_size = x.size(0)\n",
        "        sequence_length = x.size(1)\n",
        "\n",
        "        if hx is None:\n",
        "            h_t, c_t = (\n",
        "                torch.zeros(batch_size, self.hidden_size).to(x.device),\n",
        "                torch.zeros(batch_size, self.hidden_size).to(x.device),\n",
        "            )\n",
        "        else:\n",
        "            h_t, c_t = hx\n",
        "\n",
        "        output = []\n",
        "\n",
        "        for t in range(sequence_length):\n",
        "            x_t = x[:, t, :]\n",
        "            # input gate\n",
        "            i_t = torch.sigmoid(x_t @ self.W_ii + self.b_ii + c_t @ self.W_ci + self.b_ci)\n",
        "            # forget gate\n",
        "            f_t = torch.sigmoid(x_t @ self.W_if + self.b_if + c_t @ self.W_cf + self.b_cf)\n",
        "            # output gate\n",
        "            o_t = torch.sigmoid(x_t @ self.W_io + self.b_io + c_t @ self.W_co + self.b_co)\n",
        "\n",
        "            # output\n",
        "            c_t = f_t * c_t + i_t * torch.tanh(x_t @ self.W_ic + self.b_ic)\n",
        "            h_t = o_t * c_t\n",
        "\n",
        "            output.append(h_t.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat(output, dim=0)\n",
        "\n",
        "        if not self.batch_first:\n",
        "            output = output.permute(1,0,2).contiguous()\n",
        "\n",
        "        return output, (h_t, c_t)"
      ],
      "metadata": {
        "id": "tk9w7qtsOZGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\\begin{align}\n",
        "i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{ci} c_{t-1} + b_{ci}) \\\\\n",
        "f_t = \\sigma(W_{if} x_t + b_{if} + W_{cf} c_{t-1} + b_{cf}) \\\\\n",
        "o_t = \\sigma(W_{io} x_t + b_{io} + W_{co} c_{t-1} + b_{co}) \\\\\n",
        "c_t = f_t \\odot c_{t-1} + i_t \\odot tanh(W_{ic} x_t + b_{ic}) \\\\\n",
        "h_t = o_t \\odot c_t\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "WhC47nMio-QA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "a = torch.randn((5,10, 3))\n",
        "lstm = LSTMPiphole(3, 7, True)\n",
        "print(lstm(a)[0].size(), lstm(a)[1][0].size(), lstm(a)[1][1].size())\n",
        "print(lstm(a))"
      ],
      "metadata": {
        "id": "uAccfEBPyVC1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d96db7d-2212-48d6-c6ca-bd13543e176f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 5, 7]) torch.Size([5, 7]) torch.Size([5, 7])\n",
            "(tensor([[[-9.2103e-02, -2.0233e-02,  4.7249e-01, -5.2181e-01,  2.2527e-02,\n",
            "          -3.2304e-01,  5.1055e-01],\n",
            "         [ 6.5202e-01, -1.7289e-02, -2.1486e-01, -1.6997e-01,  3.2587e-02,\n",
            "           1.0264e-01,  4.0168e-01],\n",
            "         [-1.0172e-01, -5.2187e-03,  2.6390e-01, -9.2172e-01,  1.2160e-02,\n",
            "           2.4528e-02,  2.9496e-01],\n",
            "         [-1.0936e-01,  1.3243e-02, -5.5547e-02, -2.0006e-01,  7.0483e-03,\n",
            "          -6.1816e-01,  7.1652e-01],\n",
            "         [-2.8404e-02, -1.2403e-02,  6.1087e-02,  8.9413e-01,  6.6880e-02,\n",
            "           2.6533e-02,  1.9026e-01]],\n",
            "\n",
            "        [[-1.9611e-03, -8.3990e-03,  9.5282e-02, -5.9436e-02,  2.0709e-03,\n",
            "          -1.4598e-01,  1.9008e-01],\n",
            "         [ 1.5443e-01, -1.0935e-02, -2.3851e-01, -2.8001e-01,  1.0479e-02,\n",
            "           1.5410e-01,  6.8256e-01],\n",
            "         [-3.8857e-02, -2.1919e-03,  1.2440e+00, -6.0480e-01,  7.5997e-03,\n",
            "           1.3234e-02,  1.7157e-01],\n",
            "         [-3.2503e-02,  7.7197e-03,  1.6605e-01,  1.0728e-02,  1.4924e-02,\n",
            "          -3.0549e-01,  2.5479e-01],\n",
            "         [-3.1019e-02, -3.7610e-02,  1.3058e-01,  1.6237e+00,  1.0328e-02,\n",
            "           7.9672e-02,  9.5378e-01]],\n",
            "\n",
            "        [[ 1.2868e-01, -2.0617e-03,  1.3401e+00, -8.8624e-01,  9.4894e-03,\n",
            "           1.2429e-01,  7.0940e-01],\n",
            "         [ 8.3745e-02, -1.6173e-02, -2.7895e-01,  1.7441e-01,  4.7377e-03,\n",
            "           3.3523e-01,  8.6530e-01],\n",
            "         [ 6.0226e-01,  4.4460e-03,  1.7261e+00, -2.0467e-03,  2.5967e-06,\n",
            "          -2.0782e-01,  1.1567e+00],\n",
            "         [ 6.4303e-01, -4.5695e-03,  2.9227e-01, -4.8152e-01,  1.9578e-03,\n",
            "           1.9260e-01,  1.0440e-01],\n",
            "         [ 3.2126e-01,  4.4937e-02,  6.0299e-01,  3.2651e-01,  9.9766e-06,\n",
            "          -5.6825e-01,  1.6958e+00]],\n",
            "\n",
            "        [[-6.9953e-02, -3.8135e-03,  1.3384e+00, -1.6735e-01,  2.2114e-04,\n",
            "           7.5937e-03,  1.7124e+00],\n",
            "         [ 1.3174e-01, -1.8623e-02, -2.6950e-01,  8.5608e-01,  4.1753e-03,\n",
            "           5.4356e-01,  1.8208e+00],\n",
            "         [ 7.8937e-01,  1.4571e-02,  1.6446e+00,  2.3433e-01,  1.0889e-05,\n",
            "          -4.0042e-02,  1.3129e-01],\n",
            "         [ 1.3509e+00, -1.3835e-04,  3.1615e-01, -4.1565e-02,  4.3990e-04,\n",
            "          -6.7036e-01,  1.4443e+00],\n",
            "         [ 2.1541e-01,  3.5405e-01,  1.6161e+00, -1.3505e-01,  2.4279e-04,\n",
            "           1.3040e-01,  2.0435e+00]],\n",
            "\n",
            "        [[ 1.0170e-02, -3.8410e-03,  8.0300e-01,  3.9615e-01,  8.1348e-05,\n",
            "           1.5991e-02,  9.9255e-01],\n",
            "         [ 1.6701e-01, -2.3330e-02, -2.7492e-01,  1.4235e+00,  2.8099e-03,\n",
            "           1.0365e-02,  2.7874e+00],\n",
            "         [ 3.1979e-01,  5.1770e-02,  1.7166e+00, -1.2326e-01,  1.0505e-02,\n",
            "          -4.5896e-01,  1.8188e+00],\n",
            "         [ 4.8120e-02, -2.0902e-02,  3.0147e-01, -1.8248e-01,  3.1233e-05,\n",
            "          -5.1373e-01,  1.2485e+00],\n",
            "         [ 4.6786e-01,  3.5326e-02,  1.8901e+00,  1.5176e-01,  9.6553e-06,\n",
            "           9.8557e-03,  2.7223e+00]],\n",
            "\n",
            "        [[ 3.8244e-01, -6.7107e-04,  1.3038e+00, -7.3674e-02,  4.9754e-05,\n",
            "          -8.1604e-01,  3.1724e+00],\n",
            "         [ 4.1675e-01, -8.1686e-04, -2.9906e-01, -1.4499e-01,  1.8908e-06,\n",
            "           5.7652e-01,  2.8144e+00],\n",
            "         [-2.7980e-02,  7.3232e-02,  2.4316e+00, -1.0686e+00,  5.1551e-05,\n",
            "          -1.4619e-02,  4.0318e-01],\n",
            "         [ 4.1161e-02, -2.4664e-02,  1.9582e-01,  4.1111e-01,  1.4639e-04,\n",
            "          -3.3594e-01,  6.1283e-01],\n",
            "         [ 9.1660e-01,  4.5307e-03,  1.8940e+00, -4.4264e-02,  3.2378e-06,\n",
            "          -6.8301e-01,  3.8806e+00]],\n",
            "\n",
            "        [[ 3.8890e-02, -3.1989e-02,  1.2233e+00,  1.0245e-01,  1.0897e-06,\n",
            "          -1.9609e-01,  5.7182e-01],\n",
            "         [ 2.5084e-02, -4.3353e-02, -2.8582e-01, -4.1678e-01,  1.2552e-04,\n",
            "           7.7638e-02,  2.9160e+00],\n",
            "         [ 1.2731e-01,  1.5763e-02,  2.4587e+00, -1.3792e-01,  8.1691e-07,\n",
            "          -1.3637e-02,  1.5973e+00],\n",
            "         [ 1.9487e-01, -1.5923e-02,  6.3123e-01,  5.4208e-01,  2.4032e-04,\n",
            "           6.0572e-01,  9.7018e-01],\n",
            "         [ 9.0736e-01,  1.8450e-03,  1.8939e+00,  9.2938e-03,  1.8955e-08,\n",
            "          -8.9242e-01,  3.7826e+00]],\n",
            "\n",
            "        [[-9.1015e-04, -1.2794e-01,  1.9966e+00,  8.4834e-01,  2.0872e-05,\n",
            "           4.8071e-02,  1.7325e+00],\n",
            "         [ 2.5779e-01, -3.5912e-03, -3.1008e-01, -1.3272e-01,  1.3332e-05,\n",
            "          -4.7069e-01,  3.5829e+00],\n",
            "         [ 5.7588e-01,  3.5365e-03,  2.4512e+00, -4.1972e-01,  1.0838e-06,\n",
            "           2.4272e-02,  2.5797e+00],\n",
            "         [ 3.3421e-01, -4.6106e-05,  6.7899e-01, -9.0733e-03,  5.9398e-05,\n",
            "          -9.7450e-01,  3.3190e+00],\n",
            "         [ 3.7038e-01,  1.0198e-01,  1.8913e+00,  9.0030e-01,  7.0714e-08,\n",
            "          -2.8098e-01,  3.9279e+00]],\n",
            "\n",
            "        [[ 3.9560e-01, -2.7596e-03,  2.0383e+00,  6.5039e-01,  2.7876e-06,\n",
            "           2.3557e-01,  4.2167e+00],\n",
            "         [ 2.5644e-01, -5.0306e-03, -3.2648e-01, -5.9240e-02,  1.1135e-06,\n",
            "          -2.7901e-01,  2.7976e+00],\n",
            "         [ 2.9713e-01,  8.2223e-03,  2.4819e+00, -2.1041e-01,  1.8744e-07,\n",
            "           1.2647e-02,  2.0728e+00],\n",
            "         [ 5.2969e-02, -7.9165e-03,  7.5170e-01,  4.9863e-01,  1.2938e-05,\n",
            "          -4.2962e-02,  9.9050e-01],\n",
            "         [ 3.1723e-01,  8.2215e-02,  1.8812e+00,  1.6090e+00,  7.1750e-08,\n",
            "           1.6719e-01,  4.6687e+00]],\n",
            "\n",
            "        [[ 1.0426e-02, -5.3222e-02,  1.9718e+00,  1.2894e+00,  1.9014e-07,\n",
            "           1.3221e-01,  4.0282e+00],\n",
            "         [ 1.3180e-05, -6.0094e-01,  1.7355e-02,  4.7325e-01,  1.8989e-06,\n",
            "           2.3968e-02,  1.0623e-01],\n",
            "         [ 1.4832e-02,  5.3101e-02,  2.9215e+00,  4.7764e-01,  4.3563e-07,\n",
            "           3.5101e-03,  1.9046e+00],\n",
            "         [ 3.4718e-01, -1.0596e-03,  7.6203e-01, -1.9510e-01,  1.8589e-05,\n",
            "           2.7631e-01,  3.6562e+00],\n",
            "         [ 4.6657e-01,  3.5308e-02,  1.8882e+00,  1.8103e+00,  2.5562e-07,\n",
            "          -4.1295e-01,  5.5790e+00]]], grad_fn=<CatBackward0>), (tensor([[ 1.0426e-02, -5.3222e-02,  1.9718e+00,  1.2894e+00,  1.9014e-07,\n",
            "          1.3221e-01,  4.0282e+00],\n",
            "        [ 1.3180e-05, -6.0094e-01,  1.7355e-02,  4.7325e-01,  1.8989e-06,\n",
            "          2.3968e-02,  1.0623e-01],\n",
            "        [ 1.4832e-02,  5.3101e-02,  2.9215e+00,  4.7764e-01,  4.3563e-07,\n",
            "          3.5101e-03,  1.9046e+00],\n",
            "        [ 3.4718e-01, -1.0596e-03,  7.6203e-01, -1.9510e-01,  1.8589e-05,\n",
            "          2.7631e-01,  3.6562e+00],\n",
            "        [ 4.6657e-01,  3.5308e-02,  1.8882e+00,  1.8103e+00,  2.5562e-07,\n",
            "         -4.1295e-01,  5.5790e+00]], grad_fn=<MulBackward0>), tensor([[ 0.0764, -0.3384,  2.0379,  1.3396,  0.3728,  0.7280,  4.6023],\n",
            "        [ 0.0104, -0.9302,  0.1702,  0.4737,  0.7896,  0.5118,  3.1278],\n",
            "        [ 0.0855,  0.3187,  2.9256,  0.4855,  0.1115,  0.0352,  5.3598],\n",
            "        [ 0.3712, -0.0841,  0.7625, -0.2635,  0.8229,  0.3081,  3.9670],\n",
            "        [ 0.5003,  0.8033,  1.8895,  2.6649,  0.5126, -0.6200,  5.5841]],\n",
            "       grad_fn=<AddBackward0>)))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BH379E0jrBAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "937038ae"
      },
      "source": [
        "### Verification of Peephole Connections"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15623aac",
        "outputId": "35ddff5d-c68f-4273-b4d0-c3197b95a91c"
      },
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "# Instantiate the peephole LSTM\n",
        "input_size = 3\n",
        "hidden_size = 7\n",
        "lstm_peephole = LSTMPiphole(input_size, hidden_size, batch_first=True)\n",
        "\n",
        "# Create dummy input and initial hidden/cell states\n",
        "batch_size = 5\n",
        "sequence_length = 10\n",
        "x = torch.randn(batch_size, sequence_length, input_size, requires_grad=True)\n",
        "h_0 = torch.zeros(batch_size, hidden_size)\n",
        "c_0 = torch.zeros(batch_size, hidden_size, requires_grad=True)\n",
        "\n",
        "# Perform a forward pass\n",
        "output, (h_n, c_n) = lstm_peephole(x, hx=(h_0, c_0))\n",
        "\n",
        "# Perform a backward pass on an arbitrary output to get gradients\n",
        "# We'll sum the output to create a scalar for backpropagation\n",
        "output.sum().backward()\n",
        "\n",
        "# Verify that peephole weights have gradients\n",
        "print(\"Verifying gradients for peephole connections:\")\n",
        "\n",
        "# W_ci should have a gradient if c_t-1 is used in i_t calculation\n",
        "assert lstm_peephole.W_ci.grad is not None, \"W_ci did not receive gradients!\"\n",
        "print(f\"W_ci gradient is not None: {lstm_peephole.W_ci.grad is not None}\")\n",
        "\n",
        "# W_cf should have a gradient if c_t-1 is used in f_t calculation\n",
        "assert lstm_peephole.W_cf.grad is not None, \"W_cf did not receive gradients!\"\n",
        "print(f\"W_cf gradient is not None: {lstm_peephole.W_cf.grad is not None}\")\n",
        "\n",
        "# W_co should have a gradient if c_t-1 is used in o_t calculation\n",
        "assert lstm_peephole.W_co.grad is not None, \"W_co did not receive gradients!\"\n",
        "print(f\"W_co gradient is not None: {lstm_peephole.W_co.grad is not None}\")\n",
        "\n",
        "print(\"All peephole weights received gradients successfully, indicating they are part of the computation graph.\")\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verifying gradients for peephole connections:\n",
            "W_ci gradient is not None: True\n",
            "W_cf gradient is not None: True\n",
            "W_co gradient is not None: True\n",
            "All peephole weights received gradients successfully, indicating they are part of the computation graph.\n"
          ]
        }
      ]
    }
  ]
}